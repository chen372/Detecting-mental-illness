{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import defaultdict\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify dir\n",
    "Dir = 'D:/Depression_project/data/reddit'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98339"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_control = []\n",
    "with open(Dir+'/'+'randomSubreddits2017_selected.txt',\"r\") as f:\n",
    "    data = f.readlines()\n",
    "text_control = [x.strip() for x in data] \n",
    "len(text_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['want see how would address what predat eat food none charact eat meat entir movi be slightli bother bojack horseman do fantast dark episod show differ chicken eat chicken that be function member societi', 'how tall be']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['want',\n",
       " 'see',\n",
       " 'how',\n",
       " 'would',\n",
       " 'address',\n",
       " 'what',\n",
       " 'predat',\n",
       " 'eat',\n",
       " 'food',\n",
       " 'none',\n",
       " 'charact',\n",
       " 'eat',\n",
       " 'meat',\n",
       " 'entir',\n",
       " 'movi',\n",
       " 'be',\n",
       " 'slightli',\n",
       " 'bother',\n",
       " 'bojack',\n",
       " 'horseman',\n",
       " 'do',\n",
       " 'fantast',\n",
       " 'dark',\n",
       " 'episod',\n",
       " 'show',\n",
       " 'differ',\n",
       " 'chicken',\n",
       " 'eat',\n",
       " 'chicken',\n",
       " 'that',\n",
       " 'be',\n",
       " 'function',\n",
       " 'member',\n",
       " 'societi']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (text_control[:2])\n",
    "text_control[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['anybodi els look find reason live can not find',\n",
       " 'bad part be have live crazi fuck world',\n",
       " 'what do everyon do their free time summer break colleg blue seemingli my friend can not stand long summer break colleg bit month leav lot free time even work long shift week work anybodi els find case be say anybodi have idea how can fill my free time there be onli so much exercis netflix cook person can do haha']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "text_depr = []\n",
    "with open(Dir+'/'+'mentalSubreddits2017_selected.txt',\"r\") as f:\n",
    "    data = f.readlines()\n",
    "text_depr = [x.strip() for x in data] \n",
    "print (len(text_depr))\n",
    "text_depr[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words(docs):\n",
    "    words = []\n",
    "    for doc in docs:\n",
    "        for word in doc.split():\n",
    "            words.append(word)\n",
    "    print (\"The number of unique words: \"+ str(len(set((words)))))\n",
    "    return words\n",
    "\n",
    "# define a funciton to find words occurring less than n times and delete that word\n",
    "def findWordLessThan(counter, n):\n",
    "    uncommon = []\n",
    "    for k, v, in counter.items():\n",
    "        if v < n:\n",
    "            uncommon.append(k)            \n",
    "    print (len(uncommon))\n",
    "    return uncommon\n",
    "\n",
    "def findWordMoreThan(counter, n):\n",
    "    common = []\n",
    "    for k, v, in counter.items():\n",
    "        if v > n:\n",
    "            common.append(k)\n",
    "    print (len(common))\n",
    "    return common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_control = text_control[:50000]\n",
    "text_depr = text_depr[:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean control posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words: 46097\n"
     ]
    }
   ],
   "source": [
    "words_control = get_words(text_control)\n",
    "count = collections.Counter(words_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39966\n",
      "213\n"
     ]
    }
   ],
   "source": [
    "uncommon = findWordLessThan(count, 11)\n",
    "common = findWordMoreThan(count, 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-25-373bc34c067d>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-373bc34c067d>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    print (k, v)\u001b[0m\n\u001b[0m                \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\n",
    "# check\n",
    "for k, v in count.items():\n",
    "    if k in common:\n",
    "        print (k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = list(set(count.keys()) - set(uncommon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    }
   ],
   "source": [
    "messy = ['|', '@', '_', '^', 'amp;', 'gt;', 'faq_autotldr_bot/', \"](/r\", '’', '-', '█', '%', '/r', '*', '°', 'lt;3', \\\n",
    "        '#', '?', '/','‘']\n",
    "messy2 = ['a','b','c','d','e','f','g','h','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'] +\\\n",
    "['a.','b.','c.','d.','e.','f.','g.','h.','i.','j.','k.','l.','m.','n.','o.','p.','q.','r.','s.','t.','u.','v.','w.','x.','y.','z.']+\\\n",
    "[\"'\",'.','+','(', ')','*']\n",
    "messy_common = []\n",
    "for w in corpus:\n",
    "    for i in messy:\n",
    "        if w.find(i) != -1 and w not in messy2 and w.isdigit() != 1:\n",
    "            messy_common.append(w)\n",
    "print (len(messy_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6044\n"
     ]
    }
   ],
   "source": [
    "corpus0 = list(set(count.keys()) - set(uncommon) - set(messy_common))\n",
    "print (len(corpus0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flirt\n",
      "discontinu\n",
      "psychic\n",
      "cave\n",
      "classifi\n",
      "nicht\n",
      "vegan\n",
      "engin\n",
      "incur\n",
      "leather\n",
      "teller\n",
      "raspberri\n",
      "homebrew\n",
      "surviv\n",
      "eastern\n",
      "purchas\n",
      "compilebot\n",
      "thigh\n",
      "inflict\n",
      "calc\n",
      "acquir\n",
      "techi\n",
      "wander\n",
      "pen\n",
      "alex\n",
      "stud\n",
      "credenti\n",
      "mutil\n",
      "predat\n",
      "sole\n",
      "ascend\n",
      "downvot\n",
      "damper\n",
      "pedo\n",
      "obsidian\n",
      "treat\n",
      "evapor\n",
      "bottom\n",
      "fire\n",
      "dart\n",
      "genuin\n",
      "mueller\n",
      "trustworthi\n",
      "aquarium\n",
      "bicycl\n",
      "extend\n",
      "arti\n",
      "mere\n",
      "spray\n",
      "5th\n",
      "facilit\n",
      "walker\n",
      "committe\n",
      "platter\n",
      "inspect\n",
      "una\n",
      "cb\n",
      "gtx\n",
      "kkk\n",
      "bottl\n",
      "poop\n",
      "tippr\n",
      "wrap\n",
      "wikileak\n",
      "stare\n",
      "slam\n",
      "aesthet\n",
      "ni\n",
      "limb\n",
      "btw\n",
      "qui\n",
      "ahl\n",
      "teammat\n",
      "hoodi\n",
      "rite\n",
      "underdog\n",
      "m\n",
      "fring\n",
      "dong\n",
      "fluid\n",
      "sera\n",
      "over\n",
      "meetup\n",
      "arguabl\n",
      "playoff\n",
      "previous\n",
      "format\n",
      "blossom\n",
      "berri\n",
      "magnific\n",
      "butler\n",
      "temp\n",
      "etern\n",
      "printer\n",
      "custodi\n",
      "smurf\n",
      "point\n",
      "salti\n",
      "audienc\n",
      "patriarch\n",
      "jc\n",
      "manual\n",
      "vive\n",
      "maintain\n",
      "rais\n",
      "m.2\n",
      "hangout\n",
      "torqu\n",
      "swap\n",
      "peel\n",
      "dam\n",
      "ami\n",
      "cart\n",
      "consolid\n",
      "shoe\n",
      "stalker\n",
      "bush\n",
      "origin\n",
      "js\n",
      "g\n",
      "leak\n",
      "streetwear\n",
      "repeat\n",
      "lyon\n",
      "acronym\n",
      "hahaha\n",
      "dw\n",
      "billionair\n",
      "ninja\n",
      "roar\n",
      "gfe\n",
      "child\n",
      "ivanka\n",
      "dawn\n",
      "josh\n",
      "filter\n",
      "squat\n",
      "stripe\n",
      "lucid\n",
      "wave\n",
      "attack\n",
      "terrif\n",
      "choir\n",
      "sturdi\n",
      "el\n",
      "derek\n",
      "nvme\n",
      "materi\n",
      "counsel\n",
      "asian\n",
      "press\n",
      "damnit\n",
      "ha\n",
      "textbox\n",
      "ele\n",
      "ghost\n",
      "few\n",
      "feeder\n",
      "human\n",
      "slit\n",
      "p.m.\n",
      "ferguson\n",
      "0d\n",
      "lookin\n",
      "ambush\n",
      "true\n",
      "empir\n",
      "champ\n",
      "psi\n",
      "perpetr\n",
      "playground\n",
      "doggo\n",
      "independ\n",
      "capacitor\n",
      "ann\n",
      "warfram\n",
      "surgeon\n",
      "cohes\n",
      "teq\n",
      "ottawa\n",
      "usabl\n",
      "funniest\n",
      "picki\n",
      "u\n",
      "medit\n",
      "heal\n",
      "prison\n",
      "radic\n",
      "margin\n",
      "unclear\n",
      "partak\n",
      "filler\n",
      "woo\n",
      "lineup\n",
      "tone\n",
      "mat\n",
      "statement\n",
      "vastli\n",
      "keyboard\n",
      "simpson\n",
      "boss\n",
      "outlaw\n",
      "rey\n",
      "buster\n",
      "conduct\n",
      "ve\n",
      "kerr\n",
      "measur\n",
      "lebron\n",
      "xxx\n",
      "kim\n",
      "hebrew\n",
      "ani\n",
      "comeback\n",
      "slowli\n",
      "overpric\n",
      "jimmi\n",
      "psychologist\n",
      "dubiou\n",
      "ln\n",
      "breath\n",
      "valu\n",
      "parliament\n",
      "roundabout\n",
      "heatwar\n",
      "reid\n",
      "framework\n",
      "dun\n",
      "ich\n",
      "sweater\n",
      "trophi\n",
      "messag\n",
      "gestur\n",
      "ra\n",
      "kill\n",
      "conjur\n",
      "knot\n",
      "miser\n",
      "bsb\n",
      "chees\n",
      "dez\n",
      "refus\n",
      "elementari\n",
      "forget\n",
      "card\n",
      "superfici\n",
      "tug\n",
      "marketplac\n",
      "boy\n",
      "crumbl\n",
      "cod\n",
      "swat\n",
      "deploy\n",
      "skirt\n",
      "vagu\n",
      "charli\n",
      "ds\n",
      "bait\n",
      "eminem\n",
      "meat\n",
      "plant\n",
      "filthi\n",
      "recent\n",
      "al\n",
      "advisor\n",
      "ronaldo\n",
      "ansibl\n",
      "abort\n",
      "15th\n",
      "butteri\n",
      "detriment\n",
      "concert\n",
      "thailand\n",
      "idiot\n",
      "directx\n",
      "pawn\n",
      "strawberri\n",
      "mutant\n",
      "bid\n",
      "allen\n",
      "rig\n",
      "highlight\n",
      "thin\n",
      "ssj\n",
      "conserv\n",
      "verifi\n",
      "specif\n",
      "fade\n",
      "vr\n",
      "dont\n",
      "terri\n",
      "portland\n",
      "somebodi\n",
      "globaloffens\n",
      "respawn\n",
      "pose\n",
      "nixon\n",
      "exalt\n",
      "historian\n",
      "air\n",
      "internet\n",
      "fabul\n",
      "sooo\n",
      "accept\n",
      "avoid\n",
      "shorten\n",
      "smash\n",
      "anniversari\n",
      "bruh\n",
      "murphi\n",
      "lash\n",
      "girl\n",
      "ratchet\n",
      "mathemat\n",
      "hassl\n",
      "polici\n",
      "lavend\n",
      "site\n",
      "allianc\n",
      "dietari\n",
      "bulli\n",
      "hast\n",
      "convo\n",
      "sandi\n",
      "hover\n",
      "washington\n",
      "superior\n",
      "coverag\n",
      "cach\n",
      "error\n",
      "derail\n",
      "stigma\n",
      "galleri\n",
      "less\n",
      "°\n",
      "bloodborn\n",
      "lt;3\n",
      "relationship\n",
      "ethernet\n",
      "escap\n",
      "meme\n",
      "evil\n",
      "receiv\n",
      "te\n",
      "edit\n",
      "wilson\n",
      "trader\n",
      "defici\n",
      "busi\n",
      "workplac\n",
      "poetri\n",
      "phenomenon\n",
      "ptsd\n",
      "fda\n",
      "arab\n",
      "relaps\n",
      "lead\n",
      "dana\n",
      "resourc\n",
      "folk\n",
      "weak\n",
      "circumv\n",
      "lord\n",
      "gdax\n",
      "pineappl\n",
      "elia\n",
      "radiat\n",
      "applic\n",
      "chines\n",
      "thrive\n",
      "jun\n",
      "contrari\n",
      "nato\n",
      "occur\n",
      "enforc\n",
      "invas\n",
      "synergi\n",
      "undead\n",
      "same\n",
      "nepot\n",
      "victim\n",
      "fuzzi\n",
      "wiggl\n",
      "exactli\n",
      "iv\n",
      "palett\n",
      "regen\n",
      "terribl\n",
      "proton\n",
      "wacki\n",
      "robe\n",
      "grasp\n",
      "expans\n",
      "awww\n",
      "marxist\n",
      "mega\n",
      "slur\n",
      "tension\n",
      "carl\n",
      "tot\n",
      "societ\n",
      "transact\n",
      "jeremi\n",
      "credibl\n",
      "cc4\n",
      "dak\n",
      "uninterest\n",
      "broad\n",
      "ajayi\n",
      "moder\n",
      "enjoy\n",
      "win\n",
      "jungler\n",
      "blush\n",
      "button\n",
      "land\n",
      "gentl\n",
      "vagin\n",
      "waaaay\n",
      "overdo\n",
      "steadili\n",
      "iran\n",
      "sidebar\n",
      "exceed\n",
      "trickl\n",
      "defer\n",
      "infam\n",
      "routin\n",
      "invad\n",
      "collector\n",
      "utah\n",
      "gs\n",
      "spanish\n",
      "whatsapp\n",
      "pound\n",
      "kalo\n",
      "isaac\n",
      "modest\n",
      "humili\n",
      "stomach\n",
      "draymond\n",
      "optic\n",
      "nugget\n",
      "sign\n",
      "op\n",
      "gill\n",
      "moron\n",
      "behold\n",
      "unsettl\n",
      "wors\n",
      "waiver\n",
      "stranger\n",
      "perman\n",
      "pistol\n",
      "pleasur\n",
      "negoti\n",
      "json\n",
      "shit\n",
      "furri\n",
      "horseshit\n",
      "ut\n",
      "botw\n",
      "epitom\n",
      "homi\n",
      "wayn\n",
      "candid\n",
      "cabin\n",
      "tractor\n",
      "queensland\n",
      "set\n",
      "fetal\n",
      "lawrenc\n",
      "15k\n",
      "grave\n",
      "dell\n",
      "simpli\n",
      "preview\n",
      "view\n",
      "belief\n",
      "diego\n",
      "elit\n",
      "tie\n",
      "out\n",
      "variat\n",
      "eu\n",
      "under\n",
      "mug\n",
      "hilari\n",
      "k\n",
      "lifetim\n",
      "smg\n",
      "tom\n",
      "shortcut\n",
      "davidson\n",
      "forev\n",
      "footag\n",
      "shallow\n",
      "roi\n",
      "resolut\n",
      "scammer\n",
      "cylind\n",
      "metal\n",
      "instrument\n",
      "caus\n",
      "latch\n",
      "corros\n",
      "censor\n",
      "theresa\n",
      "fetch\n",
      "ford\n",
      "fine\n",
      "biblic\n",
      "hous\n",
      "legitimaci\n",
      "abstract\n",
      "level\n",
      "dramat\n",
      "nip\n",
      "occupi\n",
      "astound\n",
      "desir\n",
      "valv\n",
      "offend\n",
      "constant\n",
      "upcom\n",
      "cbd\n",
      "concuss\n",
      "untru\n",
      "detroit\n",
      "refuge\n",
      "xbox\n",
      "palestin\n",
      "casualti\n",
      "lego\n",
      "buffer\n",
      "curri\n",
      "ira\n",
      "liner\n",
      "full\n",
      "geographi\n",
      "male\n",
      "rob\n",
      "faq\n",
      "remark\n",
      "yorker\n",
      "junkrat\n",
      "gig\n",
      "chart\n",
      "mous\n",
      "dimens\n",
      "pp\n",
      "lenient\n",
      "comfort\n",
      "approxim\n",
      "anu\n",
      "blast\n",
      "centric\n",
      "warn\n",
      "ineffici\n",
      "cannabi\n",
      "simplifi\n",
      "left\n",
      "underestim\n",
      "xl\n",
      "striker\n",
      "psychopath\n",
      "holi\n",
      "her\n",
      "unpopular\n",
      "luna\n",
      "borderland\n",
      "eli\n",
      "destin\n",
      "tho\n",
      "ex\n",
      "strip\n",
      "det\n",
      "grant\n",
      "assum\n",
      "oblivi\n",
      "upfront\n",
      "align\n",
      "harp\n",
      "aliv\n",
      "maga\n",
      "tap\n",
      "payout\n",
      "transit\n",
      "kindl\n",
      "manner\n",
      "frag\n",
      "clg\n",
      "flip\n",
      "tail\n",
      "tracer\n",
      "gucci\n",
      "caveat\n",
      "tub\n",
      "sunt\n",
      "fa\n",
      "infuri\n",
      "scheme\n",
      "garden\n",
      "madam\n",
      "gravi\n",
      "self\n",
      "rest\n",
      "chile\n",
      "today\n",
      "mistaken\n",
      "hip\n",
      "hamilton\n",
      "shini\n",
      "wage\n",
      "bake\n",
      "reduct\n",
      "suburban\n",
      "tifu\n",
      "fallaci\n",
      "fraction\n",
      "musician\n",
      "paragraph\n",
      "accus\n",
      "edg\n",
      "ico\n",
      "hygien\n",
      "spiritu\n",
      "missil\n",
      "stewart\n",
      "insult\n",
      "fcc\n",
      "dim\n",
      "adblock\n",
      "lewi\n",
      "reagan\n",
      "classroom\n",
      "event\n",
      "top/?sort\n",
      "no\n",
      "lul\n",
      "pouch\n",
      "puzzl\n",
      "lab\n",
      "cork\n",
      "promin\n",
      "kh\n",
      "md\n",
      "militari\n",
      "necklac\n",
      "rapid\n",
      "brilliant\n",
      "lad\n",
      "noun\n",
      "sunshin\n",
      "cdr\n",
      "jump\n",
      "oldschoolcool\n",
      "golf\n",
      "risk\n",
      "cross\n",
      "baffl\n",
      "lod1\n",
      "perma\n",
      "cri\n",
      "defend\n",
      "pued\n",
      "gt\n",
      "agenc\n",
      "greatli\n",
      "ach\n",
      "sim\n",
      "compos\n",
      "savior\n",
      "pedophil\n",
      "tampa\n",
      "brass\n",
      "perfect\n",
      "makeup\n",
      "drive\n",
      "cereal\n",
      "exert\n",
      "sooner\n",
      "surrend\n",
      "anxieti\n",
      "talk\n",
      "restaur\n",
      "kidnap\n",
      "featur\n",
      "hmmm\n",
      "graham\n",
      "evangel\n",
      "deserv\n",
      "churn\n",
      "open\n",
      "meet\n",
      "accessori\n",
      "negat\n",
      "emp\n",
      "nigga\n",
      "ur\n",
      "selfless\n",
      "rim\n",
      "casey\n",
      "juggl\n",
      "disclos\n",
      "weekend\n",
      "webcam\n",
      "pentagon\n",
      "fault\n",
      "cd\n",
      "halo\n",
      "extern\n",
      "account\n",
      "gray\n",
      "blackmail\n",
      "quilt\n",
      "proof\n",
      "spend\n",
      "jan\n",
      "declar\n",
      "tldr\n",
      "erupt\n",
      "lust\n",
      "sushi\n",
      "sprint\n",
      "hashtag\n",
      "dehydr\n",
      "guitar\n",
      "kirk\n",
      "green\n",
      "songwrit\n",
      "tempest\n",
      "cathol\n",
      "insignific\n",
      "sorta\n",
      "colleagu\n",
      "sunni\n",
      "mayo\n",
      "allud\n",
      "retali\n",
      "constitu\n",
      "rio\n",
      "bunch\n",
      "handicap\n",
      "salah\n",
      "travi\n",
      "twitch\n",
      "cv\n",
      "imo\n",
      "laugh\n",
      "sticki\n",
      "ring\n",
      "bill\n",
      "marvin\n",
      "wast\n",
      "sallurock\n",
      "distress\n",
      "bro\n",
      "gimm\n",
      "trade\n",
      "oili\n",
      "sue\n",
      "stone\n",
      "synth\n",
      "termin\n",
      "undergradu\n",
      "best\n",
      "stalk\n",
      "assad\n",
      "resign\n",
      "jerk\n",
      "control\n",
      "spectacular\n",
      "pussi\n",
      "lili\n",
      "protagonist\n",
      "sm\n",
      "studio\n",
      "intens\n",
      "frankli\n",
      "virtual\n",
      "naw\n",
      "s5\n",
      "nash\n",
      "shock\n",
      "elderli\n",
      "skew\n",
      "nj\n",
      "stick\n",
      "liga\n",
      "pace\n",
      "wisdom\n",
      "malici\n",
      "signal\n",
      "jersey\n",
      "head\n",
      "drug\n",
      "hatch\n",
      "gel\n",
      "pregnanc\n",
      "charger\n",
      "hearsay\n",
      "undermin\n",
      "être\n",
      "flag\n",
      "tomb\n",
      "zelda\n",
      "sequel\n",
      "jackass\n",
      "tune\n",
      "promptli\n",
      "ritual\n",
      "backdoor\n",
      "summer\n",
      "changer\n",
      "cobb\n",
      "debuff\n",
      "renew\n",
      "tremend\n",
      "trinket\n",
      "allegedli\n",
      "plat\n",
      "jo\n",
      "loser\n",
      "tragic\n",
      "homophob\n",
      "royal\n",
      "dupe\n",
      "rapidli\n",
      "turk\n",
      "amber\n",
      "danc\n",
      "infest\n",
      "joy\n",
      "quot\n",
      "bamboozl\n",
      "distant\n",
      "aveng\n",
      "scorpion\n",
      "anecdot\n",
      "soon\n",
      "propel\n",
      "spiral\n",
      "temperatur\n",
      "airport\n",
      "inferior\n",
      "pluto\n",
      "membership\n",
      "manipul\n",
      "stellar\n",
      "existenti\n",
      "adress\n",
      "saber\n",
      "fate\n",
      "chain\n",
      "carbon\n",
      "vegito\n",
      "transform\n",
      "bu\n",
      "chri\n",
      "explor\n",
      "askscienc\n",
      "hypnosi\n",
      "parodi\n",
      "iso\n",
      "desktop\n",
      "#\n",
      "limit\n",
      "atheist\n",
      "order\n",
      "sc\n",
      "dna\n",
      "smug\n",
      "coin\n",
      "estat\n",
      "back\n",
      "steroid\n",
      "unleash\n",
      "snapshillbot\n",
      "search\n",
      "nap\n",
      "sewer\n",
      "paint\n",
      "india\n",
      "ako\n",
      "toll\n",
      "kinet\n",
      "static\n",
      "omit\n",
      "bradley\n",
      "father\n",
      "autonomi\n",
      "yen\n",
      "vomit\n",
      "lightsab\n",
      "iowa\n",
      "rariti\n",
      "product\n",
      "monopoli\n",
      "grammar\n",
      "mister\n",
      "appeas\n",
      "squar\n",
      "bacon\n",
      "od\n",
      "disproportion\n",
      "diagnosi\n",
      "blank\n",
      "racial\n",
      "belli\n",
      "pole\n",
      "indoor\n",
      "what\n",
      "temporarili\n",
      "intercept\n",
      "linguist\n",
      "aircraft\n",
      "roadhog\n",
      "hoa\n",
      "anyhow\n",
      "bright\n",
      "util\n",
      "persian\n",
      "episod\n",
      "room\n",
      "resubmit\n",
      "cycl\n",
      "jason\n",
      "intim\n",
      "librari\n",
      "homer\n",
      "imdb\n",
      "purg\n",
      "abid\n",
      "fake\n",
      "batteri\n",
      "wrist\n",
      "depreci\n",
      "lol\n",
      "yung\n",
      "stori\n",
      "obi\n",
      "144hz\n",
      "motif\n",
      "fund\n",
      "got\n",
      "jose\n",
      "tuck\n",
      "unnatur\n",
      "reassur\n",
      "jw\n",
      "dani\n",
      "bisp\n",
      "planetari\n",
      "usersim\n",
      "pluck\n",
      "voter\n",
      "dispel\n",
      "percept\n",
      "mint\n",
      "lsd\n",
      "hill\n",
      "50\n",
      "defeat\n",
      "sharp\n",
      "douch\n",
      "writer\n",
      "jesu\n",
      "text\n",
      "week\n",
      "must\n",
      "hace\n",
      "torrent\n",
      "frugal\n",
      "rooki\n",
      "bare\n",
      "should\n",
      "hyperbol\n",
      "crucial\n",
      "indian\n",
      "feed\n",
      "misunderstand\n",
      "robot\n"
     ]
    }
   ],
   "source": [
    "for k, v in collections.Counter(corpus0).items():\n",
    "    if k in corpus[1000:2000]:\n",
    "        print (k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean depressed posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words: 35085\n"
     ]
    }
   ],
   "source": [
    "words_depr = get_words(text_depr)\n",
    "count = collections.Counter(words_depr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23488\n",
      "2768\n"
     ]
    }
   ],
   "source": [
    "uncommon = findWordLessThan(count, 5)\n",
    "common = findWordMoreThan(count, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-37-8c6a887bd622>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-8c6a887bd622>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    print (k, v)\u001b[0m\n\u001b[0m                \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\n",
    "# check\n",
    "for k, v in count.items():\n",
    "    if k in uncommon:\n",
    "        print (k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#uncommon[3000:3100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11597\n"
     ]
    }
   ],
   "source": [
    "corpus = list(set(count.keys()) - set(uncommon))\n",
    "print (len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n"
     ]
    }
   ],
   "source": [
    "messy = ['|', '_', '^', 'amp;', 'gt;', '’', '-', '█', '%', '/r', '?', '~', '.', '/', '‘', '#']\n",
    "messy2 = ['a','b','c','d','e','f','g','h','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'] + \\\n",
    "['a.','b.','c.','d.','e.','f.','g.','h.','i.','j.','k.','l.','m.','n.','o.','p.','q.','r.','s.','t.','u.','v.','w.','x.','y.','z.']+\\\n",
    "[\"'\",'.','+','=', '―','☆', '(', ')']\n",
    "messy_common = []\n",
    "for w in corpus:\n",
    "    for i in messy:\n",
    "        if w.find(i) != -1 and w not in messy2 and w.isdigit() != 1:\n",
    "            messy_common.append(w)\n",
    "print (len(messy_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11437\n"
     ]
    }
   ],
   "source": [
    "corpus1 = list(set(count.keys()) - set(uncommon) - set(messy_common))\n",
    "print (len(corpus1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# top words in depr posts\n",
    "corpus1_new = []\n",
    "d = collections.Counter(words_depr)\n",
    "s1 = [(k, d[k]) for k in sorted(d, key=d.get, reverse=True)]\n",
    "for k, v in s1[:4000]:\n",
    "    if k in corpus1:\n",
    "        corpus1_new.append(k)\n",
    "\n",
    "# top words in control posts\n",
    "corpus0_new = []\n",
    "d = collections.Counter(words_control)\n",
    "s2 = [(k, d[k]) for k in sorted(d, key=d.get, reverse=True)]\n",
    "for k, v in s2[:3000]:\n",
    "    if k in corpus0:\n",
    "        corpus0_new.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12251\n"
     ]
    }
   ],
   "source": [
    "#corpus = corpus0_new + corpus1_new\n",
    "corpus = corpus0 + corpus1\n",
    "print (len(set(corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dic(corpus):\n",
    "    word_space=defaultdict(int)\n",
    "    for word in corpus:\n",
    "        if word not in word_space:\n",
    "            word_space[word]=len(word_space) \n",
    "    print (\"The number of unique words in the corpus is \" + str(len(word_space)))\n",
    "    return word_space\n",
    "\n",
    "def get_sparse_vec(docs, word_space):\n",
    "    sparse_vecs = []\n",
    "    for doc in docs:\n",
    "        # create empty vector\n",
    "        sparse_vec = np.zeros(len(word_space))\n",
    "        for word in doc.split():\n",
    "            if word in word_space:\n",
    "                sparse_vec[word_space[word]] = 1\n",
    "        sparse_vecs.append(sparse_vec) \n",
    "    print (\"The number of samples is \" +str(len(sparse_vecs)))\n",
    "    if len(sparse_vecs[0]) != len(word_space):\n",
    "        print (\"error\")\n",
    "    return sparse_vecs\n",
    "\n",
    "def get_freq_vec_old(docs, word_space):\n",
    "    freq_vecs = []\n",
    "    for doc in docs:\n",
    "        # create empty vector\n",
    "        freq_vec = np.zeros(len(word_space))\n",
    "        for word in doc.split():\n",
    "            if word in word_space:\n",
    "                freq_vec[word_space[word]] += 1\n",
    "        freq_vecs.append(freq_vec) \n",
    "    print (\"The number of samples is \" +str(len(freq_vecs)))\n",
    "    if len(freq_vecs[0]) != len(word_space):\n",
    "        print (\"error\")\n",
    "    return freq_vecs  \n",
    "\n",
    "\n",
    "def get_freq_vec(docs, word_space):\n",
    "    freq_vecs = []\n",
    "    for doc in docs:\n",
    "        # create empty vector\n",
    "        freq_vec = np.zeros(len(word_space))\n",
    "        for word in doc.split():\n",
    "            if word in word_space:\n",
    "                freq_vec[word_space[word]] += 1\n",
    "        freq_vecs.append(freq_vec) \n",
    "    print (\"The number of samples is \" +str(len(freq_vecs)))\n",
    "    if len(freq_vecs[0]) != len(word_space):\n",
    "        print (\"error\")\n",
    "    return freq_vecs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\"\n",
    "def get_dic(docs):\n",
    "    word_space=defaultdict(int)\n",
    "    for doc in docs:\n",
    "        for word in doc.split():\n",
    "            if word not in word_space:\n",
    "                word_space[word]=len(word_space) \n",
    "    return word_space\n",
    "\n",
    "def get_sparse_vec(doc, word_space):\n",
    "    # create empty vector\n",
    "    sparse_vec = np.zeros(len(word_space))\n",
    "    for word in doc.split():\n",
    "        try:\n",
    "            sparse_vec[word_space[word]]=1\n",
    "        except:\n",
    "            continue    \n",
    "    return sparse_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words in the corpus is 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'a': 0, 'b': 1, 'c': 2, 'd': 3})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\n",
    "m = [\"a\", 'b', 'c', 'b','d']\n",
    "c = get_dic(m)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words in the corpus is 12251\n"
     ]
    }
   ],
   "source": [
    "word_space = get_dic(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the word_space for later use\n",
    "outfile = json.dumps(word_space)\n",
    "f = open(\"D:/Depression_project/word_space.json\",\"w\")\n",
    "f.write(outfile)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get binary representation of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e1389e4ace1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msparse_vecs0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_sparse_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_control\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse_vecs0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msparse_vecs0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-b504fcd4e383>\u001b[0m in \u001b[0;36mget_sparse_vec\u001b[0;34m(docs, word_space)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[1;31m# create empty vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0msparse_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_space\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sparse_vecs0 = get_sparse_vec(text_control, word_space)\n",
    "print (len(sparse_vecs0[0]))\n",
    "print (sparse_vecs0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e558dca11f2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msparse_vecs1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_sparse_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_depr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msparse_vecs1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-b504fcd4e383>\u001b[0m in \u001b[0;36mget_sparse_vec\u001b[0;34m(docs, word_space)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[1;31m# create empty vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0msparse_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_space\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sparse_vecs1 = get_sparse_vec(text_depr, word_space)\n",
    "print (sparse_vecs1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get TF-IDF representation of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples is 50000\n",
      "12251\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "X0 = get_freq_vec(text_control, word_space)\n",
    "print (len(X0[0]))\n",
    "print (X0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples is 50000\n",
      "12251\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "X1 = get_freq_vec(text_depr, word_space)\n",
    "print (len(X1[0]))\n",
    "print (X1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X0 + X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf-idf transform\n",
    "#transformer = TfidfTransformer(use_idf=True, smooth_idf=True).fit(X)  # true means add one smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the tf-idf weights for later use (e.g., predict outcomes of new doc)\n",
    "#filename = 'D:/Depression_project/tfidf'\n",
    "#pickle.dump(transformer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection for dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def constr(rangeX=None, rangeY=None):\n",
    "    if rangeX == None and rangeY == None:\n",
    "        #X = sparse_vecs0 + sparse_vecs1\n",
    "        X = X0 + X1\n",
    "        y0 = [0 for i in range(len(text_control[:rangeX]))]\n",
    "        y1 = [1 for i in range(len(text_depr[:rangeY]))]\n",
    "    else:\n",
    "        #X = sparse_vecs0[:rangeX] + sparse_vecs1[:rangeY]\n",
    "        X = X0[:rangeX] + X1[:rangeY]\n",
    "        y0 = [0 for i in range(len(text_control[:rangeX]))]\n",
    "        y1 = [1 for i in range(len(text_depr[:rangeY]))]            \n",
    "    \n",
    "    y = y0 + y1    \n",
    "    print(len(X))\n",
    "    print (len(y))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n",
      "The number of features is 12251\n"
     ]
    }
   ],
   "source": [
    "X, y = constr()\n",
    "print (\"The number of features is \"+str(len(X[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=1000, score_func=<function chi2 at 0x000001D6AD6692F0>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the top k features\n",
    "selector = SelectKBest(chi2, k=1000)\n",
    "#selector = SelectPercentile(chi2, percentile=20)\n",
    "selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ..., False, False, False])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get parameters for this estimator.\n",
    "#selector.get_params() \n",
    "\n",
    "# Get a mask, or integer index, of the features selected\n",
    "#selector.get_support()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save selected feature to a file for later prediction use\n",
    "a = list(selector.get_support())\n",
    "f = open(\"D:/Depression_project/feature_selection.txt\", \"w\")\n",
    "for i in a:\n",
    "    if i == True:\n",
    "        f.write(str(1)+\"\\n\")\n",
    "    else: \n",
    "        f.write(str(0)+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bao\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:127: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "# fit the selected features to the data\n",
    "X_subset = selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf-idf transform\n",
    "transformer = TfidfTransformer(use_idf=True, smooth_idf=True).fit(X_subset)  # true means add one smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the tf-idf weights for later use (e.g., predict outcomes of new doc)\n",
    "filename = 'D:/Depression_project/tfidf2'\n",
    "pickle.dump(transformer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bao\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1015: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "X_input = transformer.fit_transform(X_subset)\n",
    "print (\"\\nThe number of samples in the training data is \"+str(len(X_input.toarray()))+\"\\n\")\n",
    "print (\"\\nThe number of features in model is \"+str(len(X_input.toarray()[0]))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09384399, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.20476926, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.12148246, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.28487243, 0.05853809, 0.18069294, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.32111857, 0.17321395, 0.07638137, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.15055485, 0.12374934, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\n",
    "X_train = transformer.fit_transform(X_train)\n",
    "X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\"\n",
    "a = list(selector.get_support())\n",
    "f = open(\"feature_selection.txt\" , \"w\")\n",
    "for i in a:\n",
    "    if i == True:\n",
    "        f.write(str(1)+\"\\n\")\n",
    "    else: \n",
    "        f.write(str(0)+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have our features, we can train a classifier to try to predict the category of a post. Let’s start with svm classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done fitting classifier on training data...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bao\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:127: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\n",
    "clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(linear_model.LogisticRegression(C=0.1))),\n",
    "  ('classification', svm.SVC(kernel='linear', probability=True))\n",
    "])\n",
    "clf.fit(X_train, y_train)\n",
    "print (\"\\nDone fitting classifier on training data...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svmModel = svm.SVC(kernel='linear', probability=True)\n",
    "svmModel.fit(X_input, y)\n",
    "print (\"\\nDone fitting classifier on training data...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the classifier to disk\n",
    "filename = 'D:/Depression_project/svmModel2.sav'\n",
    "pickle.dump(svmModel, open(filename, 'wb'))\n",
    "print (\"\\nModel saved to disk.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "\t accuracy_score\t 0.815\n",
      "********************\n",
      "precision_score\t 0.8620689655172413\n",
      "recall_score\t 0.75\n",
      "\n",
      "classification_report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.88      0.83       100\n",
      "          1       0.86      0.75      0.80       100\n",
      "\n",
      "avg / total       0.82      0.81      0.81       200\n",
      "\n",
      "\n",
      "confusion_matrix:\n",
      "\n",
      " [[88 12]\n",
      " [25 75]]\n"
     ]
    }
   ],
   "source": [
    "predicted = cross_validation.cross_val_predict(svmModel, X_input, y, cv=10)\n",
    "print (\"*\"*20)\n",
    "print (\"\\t accuracy_score\\t\", metrics.accuracy_score(y, predicted))\n",
    "print (\"*\"*20)\n",
    "print (\"precision_score\\t\", metrics.precision_score(y, predicted))\n",
    "print (\"recall_score\\t\", metrics.recall_score(y, predicted))\n",
    "print (\"\\nclassification_report:\\n\\n\", metrics.classification_report(y, predicted))\n",
    "print (\"\\nconfusion_matrix:\\n\\n\", metrics.confusion_matrix(y, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrModel = linear_model.LogisticRegression(random_state=0)\n",
    "lrModel.fit(X_input, y)\n",
    "print (\"\\nDone fitting classifier on training data...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the classifier to disk\n",
    "filename = 'D:/Depression_project/lrModel.sav'\n",
    "pickle.dump(lrModel, open(filename, 'wb'))\n",
    "print (\"\\nModel saved to disk.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = cross_validation.cross_val_predict(lrModel, X_input, y, cv=10)\n",
    "print (\"*\"*20)\n",
    "print (\"\\t accuracy_score\\t\", metrics.accuracy_score(y, predicted))\n",
    "print (\"*\"*20)\n",
    "print (\"precision_score\\t\", metrics.precision_score(y, predicted))\n",
    "print (\"recall_score\\t\", metrics.recall_score(y, predicted))\n",
    "print (\"\\nclassification_report:\\n\\n\", metrics.classification_report(y, predicted))\n",
    "print (\"\\nconfusion_matrix:\\n\\n\", metrics.confusion_matrix(y, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict outcome of new document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify dir\n",
    "Dir = 'D:/Depression_project/data/twitter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171514"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = []\n",
    "ids = []\n",
    "#with open(Dir+'/'+'tweets_cleaned.txt',\"r\", errors = 'ignore') as f:\n",
    "with open(Dir+'/'+'tweetsEng_cleaned_2.txt',\"r\", errors = 'ignore') as f:\n",
    "    data = f.readlines()\n",
    "texts = [x.strip().split('#^~^#')[1] for x in data] \n",
    "ids = [x.strip().split('#^~^#')[0] for x in data] \n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words: 131316\n"
     ]
    }
   ],
   "source": [
    "words = get_words(texts)\n",
    "count = collections.Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120912\n",
      "1955\n"
     ]
    }
   ],
   "source": [
    "uncommon = findWordLessThan(count, 10)\n",
    "common = findWordMoreThan(count, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10404\n"
     ]
    }
   ],
   "source": [
    "corpus = list(set(count.keys()) - set(uncommon))\n",
    "print (len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2445\n"
     ]
    }
   ],
   "source": [
    "messy_exact = ['a','b','c','d','e','f','g','h','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']+['a.','b.',\\\n",
    "'c.','d.','e.','f.','g.','h.','i.','j.','k.','l.','m.','n.','o.','p.','q.','r.','s.','t.','u.','v.','w.','x.','y.','z.']\n",
    "messy = messy_exact + [\"'\",'.','+','=', ':','rt']\n",
    "messy2 = ['#', '’', '@', 'rt', '_',  'http', '%', 'x', '…', '|', '/', '▔', 'amp', '┏', '★', '═', 'gt;',';&','lt;','】','↙'\\\n",
    "'！', '+','╝','╔', '╚', '━', '║', '『','╮','╭', '°', '―', '█','◆', '┃','「','-','╱', '┓','▏', '╗', '─',\\\n",
    "'┛','╲','」','“','┻','┗',',','?','×','#']\n",
    "messy_common = []\n",
    "for w in corpus:\n",
    "    for i in messy2:\n",
    "        if w.find(i) != -1 and w not in messy and w.isdigit() != 1:\n",
    "            messy_common.append(w)\n",
    "print (len(messy_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8324\n"
     ]
    }
   ],
   "source": [
    "corpus = list(set(count.keys()) - set(uncommon) - set(messy_common))\n",
    "print (len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get features using tf-idf representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12262\n"
     ]
    }
   ],
   "source": [
    "# load word_space \n",
    "with open(\"D:/Depression_project/word_space2.json\", mode='r') as file:  \n",
    "    word_space = json.load(file)   # json.load reads whole json file, json.loads reads string only\n",
    "print (len(word_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples is 171514\n",
      "12262\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "freq_vecs = get_freq_vec(texts, word_space)\n",
    "print (len(freq_vecs[0]))\n",
    "print (freq_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "# only include the selected features\n",
    "newX = []\n",
    "with open(\"D:/Depression_project/feature_selection.txt\", \"r\") as f:\n",
    "    reader = f.readlines()\n",
    "    for x in freq_vecs[:2000]: \n",
    "        count = 0\n",
    "        x_subset = []\n",
    "        for i in reader:\n",
    "            if i.strip() == \"1\":\n",
    "                x_subset.append(freq_vecs[count])\n",
    "            count += 1\n",
    "        newX.append(x_subset)\n",
    "print (len(newX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12251\n"
     ]
    }
   ],
   "source": [
    "# only include the selected features\n",
    "binary = []\n",
    "with open(\"D:/Depression_project/feature_selection.txt\", \"r\") as f:\n",
    "    reader = f.readlines()\n",
    "    for i in reader:\n",
    "        if i.strip() == \"1\":\n",
    "            binary.append(1)\n",
    "        else:\n",
    "            binary.append(0)\n",
    "#newX = np.array(freq_vecs) * np.array(binary)            \n",
    "#print (len(newX))\n",
    "print (len(binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\"\n",
    "# only include the selected features\n",
    "newX = []\n",
    "for i in selector.get_support():\n",
    "    if i == True:\n",
    "        newX.append(freq_vecs[i])\n",
    "print (len(newX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the tfidf result\n",
    "filename = 'D:/Depression_project/tfidf2'\n",
    "transformer = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unrecognized csr_matrix constructor usage",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\Bao\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0marg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Bao\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-2a11fa4f2b46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Bao\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[1;31m# convert counts or binary occurrences to floats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Bao\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 raise ValueError(\"unrecognized %s_matrix constructor usage\" %\n\u001b[0;32m---> 76\u001b[0;31m                         self.format)\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcoo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unrecognized csr_matrix constructor usage"
     ]
    }
   ],
   "source": [
    "X_new = transformer.transform(newX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the classifier\n",
    "filename = 'D:/Depression_project/svmModel_tfidf2.sav'\n",
    "svmModel = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 5.53827806e-01,  8.60170715e-01,  1.02946020e+00,  1.33578532e+00,\n",
       "        -3.90472004e-02,  1.03252033e+00,  1.66634532e+00,  6.92771160e-01,\n",
       "        -6.21543796e-01,  1.48108084e+00, -5.73898778e-01,  6.22504293e-01,\n",
       "        -2.83400244e-01,  3.34224049e-01,  9.44553837e-01,  1.47172234e+00,\n",
       "         1.52986850e+00,  8.03619285e-01,  5.39765086e-01, -2.67227283e-02,\n",
       "         3.42955469e-01,  1.37520748e+00,  2.29407391e+00,  3.00380250e-02,\n",
       "         3.70507545e-01, -1.36799078e-01, -9.76038941e-02,  3.47119871e-01,\n",
       "         7.46828775e-01,  1.16175737e+00,  4.65332224e-01, -9.05059946e-01,\n",
       "         1.80688434e+00,  1.73963948e+00,  7.78027282e-01,  7.51764825e-01,\n",
       "        -2.28854264e-01,  5.70453892e-01,  7.29928413e-01,  1.03956703e+00,\n",
       "        -5.18478970e-01,  5.34139297e-01,  9.44197011e-01,  2.05060722e+00,\n",
       "         1.90026631e+00,  1.43922838e+00,  1.41806093e+00, -2.01389949e-01,\n",
       "         3.72448477e+00,  2.62054448e-01,  1.59930941e-01,  8.42730128e-01,\n",
       "         1.58795198e+00,  1.15234128e+00,  3.12116385e-01,  1.54598899e+00,\n",
       "        -9.27004669e-02,  1.95923863e+00,  1.11157889e-01, -5.34966625e-01,\n",
       "         4.29224560e-01,  3.65873686e-01,  7.28723247e-01,  7.42255115e-01,\n",
       "         1.24533108e+00,  1.07102964e+00,  3.20470885e+00,  8.04753524e-03,\n",
       "         1.31453978e+00,  6.24706500e-01, -3.17850258e-01,  1.54062874e-01,\n",
       "        -1.92408202e+00,  2.12415738e+00,  1.32843850e+00,  5.53918794e-01,\n",
       "         3.27541270e+00, -2.33609759e-01,  2.19292201e-01,  1.58587511e+00,\n",
       "         1.37864969e-01,  1.28449526e+00,  1.81206850e+00,  1.00444999e+00,\n",
       "         1.35339334e+00,  7.72041542e-01, -4.57710745e-01, -7.42110551e-01,\n",
       "        -1.19341065e+00,  1.07970559e+00,  8.02285955e-02, -5.53678837e-01,\n",
       "        -7.11208132e-01,  6.87086454e-01, -4.42225050e-01,  7.55718879e-01,\n",
       "        -2.35024369e-01,  2.41418859e-01,  6.65511882e-01,  1.35298480e+00,\n",
       "         9.17509056e-01, -7.24637707e-02,  8.34543492e-01,  4.51481253e+00,\n",
       "        -8.70255049e-02, -9.07782320e-02,  1.72096829e-01,  7.92407460e-01,\n",
       "         8.44852063e-01,  6.11137573e-01,  1.75904753e+00,  1.21654816e+00,\n",
       "         7.96367777e-01,  9.86700224e-01,  1.37400179e+00, -3.34741872e-01,\n",
       "         5.00994422e-01,  5.58326019e-01, -7.83886103e-01,  7.40713155e-01,\n",
       "         4.55478604e-01,  3.24412784e-01,  8.38061737e-02, -1.36973411e+00,\n",
       "         6.64345137e-01,  6.62437144e-01,  1.15848638e+00, -2.91833394e-01,\n",
       "         7.18850722e-01,  2.20591118e+00, -2.56652507e-02,  7.90421058e-01,\n",
       "         1.39558445e+00, -9.65604959e-01,  4.01046230e-01,  7.07916898e-01,\n",
       "         1.10894179e+00,  2.97980589e+00,  4.74402481e+00,  7.26917029e-01,\n",
       "        -2.04908548e-01,  1.21610565e+00, -8.22171980e-01,  3.42149647e-01,\n",
       "         1.17075346e+00,  5.00232112e-01,  1.06500144e+00,  1.66784166e+00,\n",
       "         7.59063576e-01,  1.46651313e+00,  9.97089385e-01,  7.52206893e-01,\n",
       "         1.89980200e-01,  6.56932669e-01,  1.02319208e+00, -2.51529747e-01,\n",
       "         1.33724160e+00,  7.03542483e-02,  1.64014361e-01,  1.50806364e-01,\n",
       "        -1.35961174e-01,  5.49545709e-01,  5.53301558e+00,  6.79711202e-01,\n",
       "         6.94146355e-02,  9.26184962e-01,  9.55184079e-01,  1.68904863e+00,\n",
       "        -1.20458039e-01,  2.88322037e+00,  6.06791749e-01,  5.08855245e-01,\n",
       "        -5.32044624e-01,  1.82215221e-01,  9.93266034e-01,  8.93305019e-01,\n",
       "         2.19539129e+00,  1.74560435e+00,  7.28242142e-01, -3.38260206e-01,\n",
       "        -2.09384393e+00, -3.10930588e-01, -2.17333050e-01,  1.11684002e+00,\n",
       "        -1.59757409e-01,  3.41058462e-01,  1.24390094e-02,  6.55823036e-01,\n",
       "        -4.45076951e-01,  2.77113785e-02, -9.66192782e-01, -9.51131965e-01,\n",
       "         1.53839982e+00,  3.05898862e+00, -7.96271494e-01,  3.50782997e-01,\n",
       "        -9.53344283e-01,  2.65491853e+00,  1.18387620e+00,  4.49489267e-02,\n",
       "         1.19894305e-01,  9.76702884e-01,  6.66074899e-01,  5.35691833e-01,\n",
       "         1.28269799e+00,  5.16139577e-02,  1.45814597e+00,  8.26789471e-01,\n",
       "         1.67981760e-01,  1.01797741e+00,  5.26131574e-01,  2.04278174e+00,\n",
       "         3.27260424e-01,  3.95386293e+00, -1.89189548e-01,  6.69867936e-01,\n",
       "         8.17395270e-01, -3.44666670e-01,  1.98177007e-01,  6.60951349e-01,\n",
       "         5.46974966e-01,  1.09558903e-01,  5.97624263e-01,  5.10994220e-01,\n",
       "         2.76792091e-01,  3.41320232e+00,  4.29901953e-01,  1.00836414e+00,\n",
       "        -3.91148887e-01,  3.59728545e-01,  1.37851377e+00,  6.72209804e-01,\n",
       "         9.38985917e-02,  2.76923525e-01,  2.34043349e-01,  3.93833468e-01,\n",
       "        -4.33925462e-01,  2.13244519e-01, -2.59112184e-01, -1.84784209e-01,\n",
       "         4.91486661e-01,  4.58021215e-01,  1.03630988e+00, -9.49960437e-01,\n",
       "         1.08478638e+00,  1.02802799e+00, -4.10356559e-01,  7.33852976e-01,\n",
       "         9.02082996e-01,  1.00924522e+00,  3.48217438e-01,  4.55911373e-01,\n",
       "         9.36540249e-01,  8.31194585e-01,  7.85831522e-01, -6.19830086e-01,\n",
       "         2.45902868e+00,  9.26033942e-01,  1.35318612e+00,  2.42619218e-01,\n",
       "         1.77527129e-01,  1.33635078e+00,  1.63776315e-02,  4.69632195e-01,\n",
       "        -1.42019669e-01,  6.20052784e-03,  4.60082660e-01,  1.88100909e+00,\n",
       "        -9.64703937e-01,  2.73382334e+00,  5.01736292e-01,  1.52917254e+00,\n",
       "        -2.05394526e-01,  9.59400465e-01,  7.73789940e-01,  7.69618559e-01,\n",
       "         1.39767444e-01,  1.36246144e+00,  4.86026010e+00,  1.43431346e+00,\n",
       "         2.87893344e-01,  1.35335504e+00,  1.81326709e-01,  3.79969603e-01,\n",
       "         1.15766625e+00,  1.35295502e+00,  6.33513879e-01,  8.84796494e-01,\n",
       "        -8.32819031e-02, -5.65824327e-02, -4.99539205e-01,  2.90819919e+00,\n",
       "         1.67646355e+00, -2.44633454e-02,  8.39331291e-01,  1.90586386e+00,\n",
       "         4.69655941e-01,  3.70083910e-01,  1.35280873e+00,  6.93748162e-01,\n",
       "         8.21691442e-01,  5.30121821e-01,  9.59754870e-01, -4.19092514e-01,\n",
       "        -8.79097013e-02,  9.56923622e-01,  5.27634980e-01,  1.98569157e+00,\n",
       "         6.95817305e-01, -9.12754079e-02, -2.40556812e+00,  2.82613024e-01,\n",
       "         7.40921143e-01, -5.04578285e-01,  1.35286517e+00,  3.07477233e+00,\n",
       "         2.33086967e-01, -1.70808534e+00,  8.53465124e-01,  3.34117008e-01,\n",
       "         1.68183441e-01,  1.17575203e+00,  1.35324278e+00, -7.10602949e-01,\n",
       "         6.59963078e-01, -2.81732145e-01,  1.26294719e+00,  7.59693997e-01,\n",
       "        -3.74816201e-01,  8.90290324e-02,  1.16684210e+00,  1.37936691e-02,\n",
       "         2.82917168e-01,  9.58079283e-01,  5.15895700e-01,  2.68567211e+00,\n",
       "        -1.37442195e-01,  1.35281617e+00,  4.92029226e-01,  2.63484933e-01,\n",
       "         2.45239923e+00,  3.78795982e-02, -7.90202389e-01,  2.84267935e-01,\n",
       "         5.77355451e-01,  1.31552035e+00,  1.31726029e+00, -9.23031658e-02,\n",
       "         1.82915945e+00,  2.05152154e+00,  4.48215224e-01,  2.00209544e-01,\n",
       "         2.05443540e-01,  3.42164083e-01, -3.19716605e-01,  3.70143698e+00,\n",
       "         1.76095958e-01,  2.94695410e+00,  1.48583649e+00, -9.57018812e-01,\n",
       "         7.36757935e-01,  4.18603860e+00, -4.27983103e-01,  8.54742421e-01,\n",
       "        -2.05013230e-01,  7.55667132e-01, -1.23420148e+00, -7.03693288e-01,\n",
       "         6.59671879e-01,  1.11948217e+00, -5.90232838e-01,  5.13297679e-02,\n",
       "         1.90159028e+00,  2.66587575e-01, -4.52183376e-01,  3.78821664e-01,\n",
       "         9.66008302e-01,  2.41205931e+00,  1.00252916e-01,  3.36495079e-01,\n",
       "         2.02520353e+00,  5.41181086e-01, -4.34260715e-01,  2.02146420e-01,\n",
       "         6.87276860e-01, -3.56650203e-01,  8.96953557e-01, -5.48762325e-03,\n",
       "         1.15470750e+00,  6.66134758e-01,  2.79719093e+00,  9.67079427e-01,\n",
       "         1.28944846e+00,  9.74428054e-01,  2.57189434e-01,  8.06235238e-03,\n",
       "         1.47874995e+00,  1.29124300e-01, -2.82325862e-01,  4.58337278e-01,\n",
       "         2.10782966e+00,  2.07765687e+00,  8.66086536e-01,  1.18339402e+00,\n",
       "         1.29413045e+00,  2.95232605e-01,  1.03373764e+00,  4.07959933e-01,\n",
       "         5.17200281e-02,  1.22919731e+00,  3.50959593e-02,  1.20119280e+00,\n",
       "        -2.61465565e-01,  1.79522837e+00,  1.09809102e+01,  1.42793250e+00,\n",
       "         1.00090153e+00,  3.23545145e-01,  1.33053794e+00, -7.64576950e-01,\n",
       "         4.79523391e+00, -2.94254283e-02,  1.48859783e+00, -2.23045282e-01,\n",
       "        -3.80681838e-01,  1.27520109e+00, -3.29473330e-01, -3.75464717e-01,\n",
       "         6.94372599e-01,  1.48491815e-01,  7.62719448e-01,  1.35273572e+00,\n",
       "         4.38251175e+00,  1.18564827e+00,  5.48850458e-02,  1.58372308e+00,\n",
       "         2.47430372e-01, -7.74252597e-01,  1.33345626e+00,  2.70927371e+00,\n",
       "        -1.88415767e-01, -1.78869926e-01,  7.71622691e-01,  1.79613447e+00,\n",
       "         2.58895220e-01,  1.03848286e+00, -3.78397875e-02,  2.16842640e+00,\n",
       "         1.14846644e+00,  3.49969469e-01,  1.21754290e+00,  3.14698154e-01,\n",
       "         1.75157928e-01,  9.73162037e-01,  6.04334867e-01,  2.78253944e+00,\n",
       "         7.31387613e-01,  6.26383276e-01,  7.71150745e-02,  1.88553349e+00,\n",
       "         1.63402903e-01, -7.10673829e-01,  1.14526832e+00, -1.53272616e+00,\n",
       "         4.78108254e-01, -1.14058130e-01,  2.25511978e-01, -1.46936025e-02,\n",
       "         1.18957815e-01,  1.44881908e-01, -4.27988905e-01,  9.12941286e-01,\n",
       "         8.41450439e-01,  1.53025807e+00, -1.43973030e-01,  4.68734598e+00,\n",
       "        -5.08844646e-02,  9.40635888e-01,  1.65792795e+00,  1.15690095e+00,\n",
       "        -3.62069153e-01,  9.22156113e-02, -7.58149433e-01,  1.53783419e+00,\n",
       "         1.33519097e-01,  1.47896177e+00, -3.27338893e-01,  7.90931711e-01,\n",
       "         7.79851732e-02, -2.89056296e-01,  1.08313204e+00,  1.68630464e+00,\n",
       "        -2.76779380e-01, -5.06125543e-01, -4.12936941e-01, -6.03816517e-01,\n",
       "        -6.74233939e-01,  1.72609811e+00,  1.14659560e+00,  6.46782138e-01,\n",
       "         1.21527585e+00, -1.09258461e-01,  1.55136686e+00, -5.47062992e-01,\n",
       "         1.34588452e+00,  3.35281933e+00,  1.17061594e-02,  1.35281567e+00,\n",
       "        -1.00263556e-01,  1.86502960e+00,  7.08115344e-01, -3.42878179e-01,\n",
       "         9.58018695e-01,  5.76691116e+00,  1.91035496e+00,  1.09059691e+00,\n",
       "         4.80965351e+00,  1.57169400e+00,  1.31261252e+00,  4.42836166e-01,\n",
       "         1.64454357e+00, -2.00722016e-01, -6.86951543e-01,  7.48057520e-01,\n",
       "         2.93316694e+00,  3.48194162e+00,  5.46911467e-01,  2.60331034e+00,\n",
       "        -1.29057407e-01,  4.13000636e-01, -1.74098812e-02, -3.09585250e-01,\n",
       "         4.83559738e-01,  1.18776185e+00,  5.07486304e-01,  1.47844303e+00,\n",
       "         3.58081908e-01,  7.92318484e-01,  2.83551029e+00, -3.47727444e-01,\n",
       "         1.50459837e+00,  5.78663804e-01,  3.92852292e-01, -6.32191488e-01,\n",
       "         6.21440509e-02, -3.77802770e-01, -1.65756547e-01,  1.32566444e+00,\n",
       "        -1.84207685e-01, -7.94601258e-01,  1.83312235e+00,  1.12829353e+00,\n",
       "         3.45688304e-01,  1.91087709e-01,  4.78123741e-01,  7.32117696e-01,\n",
       "         2.09121722e+00,  8.60292079e-01,  1.17039019e+00,  3.88820226e-01,\n",
       "        -9.53827150e-02,  1.10983831e+00,  1.16305811e+00, -3.33664408e-01,\n",
       "         1.03884063e+00,  9.44955331e-01,  7.22782666e-01,  1.23610448e+00,\n",
       "        -5.65579629e-01,  8.51187292e-01,  1.15725866e+00,  2.14634947e+00,\n",
       "        -3.29481294e-01,  1.69018893e+00, -4.78862658e-01,  3.81739628e-01,\n",
       "         2.53988982e+00,  1.50558408e+00,  2.61762341e+00,  3.22938497e-01,\n",
       "        -4.99088922e-01, -3.13547890e-01, -2.44246079e-01,  2.79142356e+00,\n",
       "         1.03860355e+00,  2.39943608e+00, -4.58379421e-01,  8.21021921e-01,\n",
       "         1.88463919e-01,  1.13438353e+00,  1.43065669e+00,  9.46030014e-01,\n",
       "        -1.09884477e-01,  7.67128647e-01,  6.89285497e-01,  2.25639863e-01,\n",
       "         3.89927629e-02,  6.35590879e-01,  3.68314966e-01, -5.62294291e-01,\n",
       "         4.51999665e-01,  3.39927657e+00, -4.89639186e-01, -2.24628189e-01,\n",
       "         2.25946621e+00,  3.40701117e-01,  5.62216044e-01, -2.63354180e-01,\n",
       "         8.66848564e-01,  6.10778421e-01,  7.81360150e-01,  6.40834381e-02,\n",
       "         2.58513702e-01,  1.01554459e+00,  2.81450293e+00,  1.77025672e+00,\n",
       "        -8.43965387e-01,  1.08197884e-01,  8.48446103e-01,  1.50711813e+00,\n",
       "         1.04546895e+00, -3.10237155e+00,  5.69291841e-01,  2.60175637e-01,\n",
       "         1.11128091e+00,  1.13828577e+00,  5.20355315e-01,  2.33428339e+00,\n",
       "        -5.94617843e-01,  4.35636951e+00,  8.77778037e-01,  1.15959820e+00,\n",
       "         1.61667311e+00,  1.24292502e+00,  9.63879255e-01,  6.58709206e-01,\n",
       "        -5.51832485e-01, -2.95617783e-01, -4.08572414e-01,  8.57039858e-01,\n",
       "         4.44810451e-01,  5.23953287e-01,  3.69055816e-01,  1.17389479e+00,\n",
       "         6.71977432e-01,  1.05244366e+00,  4.36631351e-01, -7.95171551e-02,\n",
       "        -1.97225516e-03,  1.76238463e+00,  1.49245744e+00,  2.23367397e+00,\n",
       "         2.38175784e+00,  1.21836350e+00,  1.28591344e+00,  2.70885009e-01,\n",
       "         3.36088146e+00,  1.66200013e-01,  1.12482219e+00,  1.47275838e-01,\n",
       "        -9.62759133e-01,  3.70632554e-01,  1.54856195e+00, -8.82010891e-01,\n",
       "        -5.57282669e-01,  8.42219643e-01, -1.31027224e-01,  2.15848847e+00,\n",
       "         9.10780117e-01,  8.55917903e-01, -7.17493945e-02,  1.43169487e+00,\n",
       "         2.46939837e+00,  2.00340982e-01,  7.40390615e-01,  4.07593624e-01,\n",
       "         1.27800758e+00,  6.03843406e-01,  6.27408958e-01,  8.29661938e-01,\n",
       "         8.22078704e-01, -2.95264515e-01,  5.18491027e-01,  1.11725657e+00,\n",
       "        -2.62150314e-01, -6.77429799e-02,  1.74211770e+00,  6.22919223e-02,\n",
       "         3.74783963e-03,  6.72325912e-01,  4.22291998e-01,  6.24955166e-01,\n",
       "         4.34841764e-01,  1.25466427e+00,  1.33143252e+00,  1.15114460e+00,\n",
       "        -4.51765380e-01,  1.06811342e+00,  1.24739246e+00,  3.91129179e+00,\n",
       "         2.66179466e+00,  1.14682363e-01,  3.26306514e-01,  1.09985959e+00,\n",
       "         7.65724252e-01,  4.20204694e-01,  8.14902515e-01,  3.63961235e-01,\n",
       "         2.66525165e-01,  1.17271445e+00, -1.20613609e+00, -2.58103066e+00,\n",
       "        -5.87065963e-01, -4.23314410e-01,  1.84655586e+00, -2.21445550e-02,\n",
       "         9.19214782e-01,  5.18691779e-01,  1.36540865e-01,  5.79851855e-01,\n",
       "         9.23756993e-01,  5.05122769e-01,  5.77942222e-01,  9.79450081e-02,\n",
       "         1.34249350e+00, -8.25425423e-01,  6.00471310e-01,  1.46860524e+00,\n",
       "         7.12423568e-01,  2.60038045e-01,  5.28372650e-01,  2.61549046e+00,\n",
       "         9.85685049e-01,  1.48428837e+00,  1.11233064e+00,  2.85732760e-01,\n",
       "         1.27320938e+00,  4.57405957e-01,  9.15212672e-02,  1.29938363e+00,\n",
       "         7.44205459e-01,  6.95616711e-02,  5.37457603e-01,  1.68940813e-01,\n",
       "         7.80609037e-01,  1.54913738e+00,  1.59950958e+00,  7.70012106e-01,\n",
       "         1.35321035e+00, -4.84607471e-01, -7.23850811e-02,  2.39391564e+00,\n",
       "         7.84066542e-01,  7.45637218e-01,  7.39502447e-01,  9.67410966e-01,\n",
       "         8.75724688e-01, -4.55230062e-01,  2.09768830e+00,  1.06946446e+00,\n",
       "         6.33726993e-01, -1.76384482e-01,  1.12606507e+00, -1.96498448e-01,\n",
       "         5.63298485e-01,  9.42910611e-01,  2.12405876e-01,  1.51540914e+00,\n",
       "        -8.18926125e-01,  1.43648927e+00,  2.29377348e-01, -2.84729003e-01,\n",
       "         9.12217965e-01,  1.14365735e+00,  2.37363456e+00, -3.85726975e-01,\n",
       "         3.23791957e-01,  9.34471043e-01,  3.31137791e-01, -4.78829360e-02,\n",
       "        -2.21785504e+00,  4.24485436e-01,  5.97148848e-01, -1.04280179e+00,\n",
       "        -2.49250603e-01,  1.15217941e+00,  1.46589703e+00,  1.08906483e+00,\n",
       "         6.52945425e-02,  1.07895175e+00,  2.10384045e+00,  1.11848932e-01,\n",
       "         6.99879565e-03,  2.11760719e+00, -2.78770944e-01,  8.34441781e-01,\n",
       "         6.35706591e-01, -2.43211849e-01, -6.35486906e-01,  2.48737975e-01,\n",
       "         3.26948206e-01, -5.13292373e-02,  6.65299855e-01,  3.63432666e+00,\n",
       "         4.47342566e-01,  4.82898745e-01, -8.42195353e-01, -3.18323145e-02,\n",
       "        -1.85918724e-02,  1.25906480e+00,  2.68356510e-01, -3.69483728e-01,\n",
       "         1.39931226e+00,  4.66028527e-01,  1.25569100e+00,  4.55520985e-01,\n",
       "         2.44832363e-02,  1.02866753e-01, -1.71344253e-01,  8.07895732e-01,\n",
       "         3.63861141e-01, -7.15574996e-01,  1.35278237e+00,  8.10143039e-01,\n",
       "        -3.13728088e-01, -6.92492920e-01, -3.91543815e-01, -1.17091487e+00,\n",
       "         6.24816436e-01,  1.50180085e+00,  2.57977990e+00,  8.12976495e-01,\n",
       "        -1.55518085e-01,  7.68944787e-01,  9.00624723e-01,  1.46124932e-01,\n",
       "         5.61034741e-01,  2.79866782e+00,  1.35323682e+00,  4.08162758e-01,\n",
       "         8.44818803e-01,  5.01334305e-01,  1.95417691e-01,  1.76107980e-01,\n",
       "        -4.29028443e-02,  8.69731343e-01,  1.10892327e+00,  3.02215107e-01,\n",
       "         1.16354480e+00,  9.26251832e-01, -2.85359941e-01,  1.50747708e-01,\n",
       "         6.88423777e-01,  2.80185628e+00, -1.84204095e+00,  3.09514802e-01,\n",
       "        -1.99168059e-01,  1.42727577e+00,  8.52409508e-01,  9.01991129e-01,\n",
       "         3.72189963e-01, -7.47060458e-01,  1.18321865e+00, -5.46084067e-01,\n",
       "         7.68102868e-01,  7.07181630e-01,  5.34138633e-01,  1.75194412e+00,\n",
       "         4.36330374e-02,  6.60982972e-02,  7.16372245e-01,  3.05329256e+00,\n",
       "         1.62760331e-02,  8.46073853e-01,  9.32228824e-01,  9.16281069e-01,\n",
       "        -8.05853132e-01,  9.79085765e-02,  4.29835782e-01,  8.57558644e-02,\n",
       "        -1.31333276e-01,  1.61441364e-01, -6.51243555e-01, -5.47603111e-01,\n",
       "         1.35317064e+00,  3.09668286e-02, -8.62859665e-01,  4.78140584e-01,\n",
       "         2.27964860e-01,  5.16607213e-02,  1.84860910e+00,  2.37584411e-01,\n",
       "        -2.80466310e-01,  5.68252390e-01,  1.73251347e+00,  5.91275877e-01,\n",
       "         1.83350543e+00,  1.63779845e+00, -1.08033989e+00,  2.69500780e+00,\n",
       "         1.04322047e+00,  5.57104193e-01,  1.35890178e-01, -4.72113046e-01,\n",
       "         3.35861516e-01,  4.06981156e+00,  2.73679700e-01,  7.13126160e-01,\n",
       "         5.14816362e-02,  2.18372511e-01,  4.08007356e-01, -2.74645308e+00,\n",
       "         6.34870366e-02, -1.80730747e+00, -2.09730186e-01, -6.28879381e-01,\n",
       "         6.70399752e-02,  1.53303141e+00,  1.28083644e+00,  2.45131239e+00,\n",
       "         1.23857122e+00, -2.25901893e-01,  4.46140964e-01,  9.02558774e-01,\n",
       "        -8.64401211e-01,  2.11301426e-02,  1.24082563e+00,  1.31744118e+00,\n",
       "         4.54481983e-02,  8.06832659e-01, -2.39349890e-02,  1.70847654e-01,\n",
       "         2.21499790e+00,  8.81000200e-01, -1.23562546e-01,  1.79915265e-01,\n",
       "         1.49481343e+00,  1.35084902e+00, -6.54315033e-01,  3.98562331e-01,\n",
       "         9.98954417e-01,  2.13064699e+00,  1.00443757e+00,  1.05848191e-02,\n",
       "         8.92089562e-01,  9.88962197e-01,  1.44669780e+00, -5.45196542e-01,\n",
       "         8.35987891e-01,  8.61777804e-01,  1.83094201e+00, -4.62233962e-01,\n",
       "         4.03079146e-01,  1.18172219e+00,  1.16423430e+00,  4.32953274e-01,\n",
       "         2.25890912e+00,  3.07530796e+00,  1.14624165e+00,  1.90455729e+00,\n",
       "         3.68382086e+00,  1.06924524e+00,  4.09134476e+00,  3.35338075e+00,\n",
       "         6.92523531e+00,  2.07602811e+00,  2.77242185e+00,  1.51871065e+00,\n",
       "         3.68827947e+00,  3.55451409e+00,  4.23347031e+00,  2.96464090e+00,\n",
       "         1.87537125e+00,  8.99352951e+00,  3.58793976e+00,  2.38161658e+00,\n",
       "         8.66320988e-01,  3.63770228e+00,  3.63567933e+00,  3.28329262e+00,\n",
       "         3.42203434e+00,  3.57296860e+00,  4.27280907e+00,  2.25881083e+00,\n",
       "         1.21679061e+00,  3.47347842e+00,  2.07132651e+00,  3.19854934e+00,\n",
       "         5.83543605e+00,  4.68662857e+00,  2.28684361e+00,  3.15456737e+00,\n",
       "         3.25226006e+00,  2.11449872e+00,  2.48971521e+00,  2.17123029e+00,\n",
       "         5.22898816e-01,  2.33971700e+00,  2.76613644e+00,  4.45900402e+00])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmModel.coef_.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[8, 21, 27, 39, 40, 44, 45, 47, 50, 59]\n"
     ]
    }
   ],
   "source": [
    "feature_index = []\n",
    "count = 0\n",
    "for i in binary:\n",
    "    if i == 1:\n",
    "        feature_index.append(count)\n",
    "    count += 1\n",
    "print (len(feature_index))\n",
    "print (feature_index[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "['misread', 'sugar', 'veget', 'well', 'dri', 'empir', 'note', 'allow', 'difficult', 'suffic']\n"
     ]
    }
   ],
   "source": [
    "feature_show = []\n",
    "for i in feature_index:\n",
    "    for k, v in word_space.items():\n",
    "        if i == v:\n",
    "            feature_show.append(k)\n",
    "print (len(feature_show))\n",
    "print (feature_show[:10])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['misread', 'sugar', 'veget', 'well', 'dri', 'empir', 'note', 'allow', 'difficult', 'suffic', 'whi', 'potter', 'frost', 'show', 'instal', 'territori', 'injustic', 'reduc', 'cd', 'splash', 'histori', 'jay', 'throat', 'stink', 'bolt', 'hardwar', 'bargain', 'forest', 'comput', 'urban', 'deliv', 'steve', 'singer', 'vote', 'rad', 'underwear', 'turtl', 'rememb', 'stay', 'har', 'vm', 'therapi', 'upvot', 'closer', 'shame', 'australian', 'chore', 'final', 'rampant', 'short', 'recognit', 'lisa', 'marri', 'l.', 'off', 'temp', 'pepe', 'sister', 'pedal', 'cooper', 'l', 'fox', 'treati', 'elsewher', 'biscuit', 'draw', 'be', 'dinner', 'sweater', 'freezer', 'discord', 'medicin', 'cardio', 'guidelin', 'pg', 'sarcasm', 'britain', 'nice', 'quick', 'acronym', 'element', 'border', 'franchis', 'glow', 'intim', 'pixel', 'nazi', 'friday', 'vietnam', 'mad', 'subscript', 'tini', 'everyth', 'lanc', 'standard', 'produc', 'modern', 'surprisingli', 'messag', 'wii', 'gamma', 'approach', 'ms', 'alloc', 'roll', 'taylor', 'much', 'station', 'variou', 'esp', 'fifa', 'mr.', 'mother', 'templ', 'shin', 'alt', 'acn', 'head', 'ward', 'pouch', 'bingo', 'cigarett', 'safeti', 'intend', 'tabl', 'primal', 'further', 'rb', 'nhl', 'mandatori', 'preorder', 'area', 'bundl', 'offend', 'login', 'blaster', 'meet', 'clich', 'copyright', 'rope', 'hike', 'sentiment', 'degre', 'freak', 'period', 'matt', 'coin', 'skirt', 'recip', 'regim', 'suck', 'parent', 'ethereum', 'refus', 'rebellion', 'amazingli', 'econom', 'nippl', 'thi', 'dice', 'wifi', 'haircut', 'mock', 'otherwis', 'nottheonion', 'congratul', 'empathi', 'salmon', 'fad', 'rid', 'headphon', 'drain', 'outfit', 'pocket', 'alter', 'suspens', 'compil', 'gate', 'regularli', 'ani', 'brawl', 'casino', 'opinion', 'cj', 'hourli', 'dc', 'orient', 'edgi', 'cours', 'cathol', 'huh', 'unattract', 'accent', 're', 'fabric', 'ss', 'disast', 'sl', 'tale', 'optic', 'villag', 'semi', 'wilson', 'dishonest', 'merchant', 'reinforc', 'technic', 'habit', 'lake', 'incorrectli', 'sunday', 'playoff', 'commit', 'repay', 'euro', 'scienc', 'sick', 'inclus', 'posit', 'zeke', 'freelanc', 'inher', 'nativ', 'instagram', 'enorm', 'notch', 'brexit', 'totesmesseng', 'heal', 'toronto', 'either', 'offic', 'funni', 'unusu', 'es', 'shitpost', 'cheek', 'fucker', 'forgot', 'iran', 'publicli', 'clearanc', 'wikipedia', 'line', 'reign', 'jailbreak', 'imguralbumbot', 'blockchain', 'harmoni', 'chick', 'iter', 'belief', 'terrain', '5th', 'millenni', 'blu', 'betray', 'grandpar', 'repeat', 'pilot', 'overweight', 'leg', 'ink', 'paypal', 'public', 'disadvantag', 'appl', 'star', 'referendum', 'materi', 'fool', 'cart', 'appreci', 'investor', 'hater', 'coup', 'pot', 'singapor', 'philosoph', 'real', 'land', 'associ', 'tune', 'grass', 'mood', 'tradit', 'insid', 'realm', 'utah', 'sweet', 'fair', 'bacteria', 'wwii', 'injuri', 'wound', 'poorli', 'congrat', 'ryzen', 'quiet', 'clash', 'ron', 'analysi', 'europ', 'carpet', 'wiki', 'bunch', 'est', 'curri', 'angl', 'analog', 'mp', 'fuck', 'chew', 'emul', 'ugli', 'autonom', 'bold', 'kendrick', 'toddler', 'runner', 'maximum', 'answer', 'machin', '4th', 'jealou', 'poor', 'silli', 'warrant', 'assum', 'mmr', 'shake', 'mess', 'miner', 'experienc', 'sniff', 'ssr', 'na', 'span', 'flash', 'lurk', 'boot', 'shelter', 'classic', 'fyi', 'hypothet', 'hollow', 'embrac', 'sander', 'heavi', 'where', 'templat', 'hd', 'purpl', 'ador', 'ist', 'derail', 'valid', 'shot', 'thursday', 'tantrum', 'snail', 'kylo', 'medit', 'god', 'alex', 'rich', 'desper', 'delus', 'quot', 'greet', 'cream', 'lay', 'asid', 'describ', 'scientif', 'omg', 'level', 'mind', 'marvel', 'axe', 'bucket', 'georgia', 'do', 'warm', 'legit', '+', 'athlet', 'novemb', 'paus', 'inner', 'warranti', 'recreat', 'structur', 'uniqu', 'coalit', 'queue', 'hint', 'british', 'ib', 'drone', 'jaw', 'gap', 'dr', 'choic', 'portug', 'interpret', 'aid', 'donat', 'preciou', 'statu', 'campu', 'trader', 'ski', 'jim', 'commerci', 'doll', 'civil', 'senior', 'poli', 'unreal', 'bio', 'success', 'educ', 'rebuild', 'librari', 'due', 'treatment', 'nobodi', 'convo', 'blender', 'continu', 'viewer', 'calib', 'addit', 'cal', 'spike', 'kidnap', 'selfish', 'astro', 'maxim', 'aw', 'appropri', 'nova', 'similar', 'soccer', 'breakfast', 'street', 'houston', 'repeal', 'realiz', 'glad', 'mic', 'wtf', 'jew', 'influenc', 'water', 'armi', 'festiv', 'stud', 'onward', 'neoliber', 'scheme', 'lifestyl', 'risk', 'duel', 'earli', 'halloween', 'cotton', 'yeh', 'grab', 'xd', 'radar', 'contest', 'lime', 'complain', 'repost', 'flee', 'swamp', 'liter', 'settl', 'state', 'akin', 'revolv', 'turbo', 'admittedli', 'surg', 'gradual', 'coincident', 'spell', 'argument', 'zarya', 'intrigu', 'coffe', 'gdp', 'citizenship', 'ether', 'sexist', 'till', 'parasit', 'sustain', 'subconsci', 'rein', 'newspap', 'coinbas', 'eu', 'indoor', 'divis', 'doo', 'immun', 'asset', 'sub', ':)', 'bc', 'tutori', 'creat', 'damag', 'valley', 'crusad', 'shove', 'three', 'downgrad', 'tour', 'comeback', 'ridicul', 'crew', 'packag', 'complic', 'bs', 'train', 'transmiss', 'merced', 'justic', 'hydra', 'shi', 'jimmi', 'gi', 'graphic', 'full', 'use', 'deliber', 'iraq', 'mandat', 'float', 'capitalist', 'leader', 'jesu', 'hole', 'modifi', 'keyword', 'negoti', 'bleach', 'nixon', 'flagel', 'legaci', 'microwav', 'rainbow', 'evil', 'dome', 'thin', 'mall', 'harden', 'rule', 'stephen', 'interact', 'rampag', 'occupi', 'challeng', 'violat', 'siren', 'rel', 'fuel', 'broken', 'du', 'town', 'flag', 'space', 'tuition', 'volum', 'bachelor', 'matrix', 'cloud', 'stream', 'psych', 'reliabl', 'eye', 'rack', 'manag', 'h', 'dunk', 'anim', 'close', 'dumbass', 'ted', 'playlist', 'romanc', 'incorrect', 'burst', 'c', 'financ', 'complianc', 'frustrat', 'press', 'probabl', 'illustr', 'claim', 'rice', 'melt', 'crop', 'eyebrow', 'potu', 'man', 'farmer', 'whine', 'requir', 'sunset', 'dnc', 'current', 'dak', 'spam', 'should', 'abov', 'highlight', 'ideolog', 'jewish', 'toe', 'better', 'swell', 'intoler', 'quit', 'redeem', 'expos', 'metric', 'cc2', 'ult', 'add', 'conceiv', 'spotifi', 'react', 'triangl', 'buzzfe', 'steal', 'sea', 'rental', 'idea', 'epic', 'bribe', 'north', 'pump', 'wr', 'narr', 'secondli', 'it', 'flair', 'gravi', 'half', 'offer', 'activ', 'planner', 'tow', 'pull', 'transform', 'disco', 'ssd', 'navi', 'normal', 'dr.', 'electron', 'locker', 'night', 'winner', 'regist', 'piec', 'j', 'c.', 's2', 'puff', 'luck', 'aspir', 'often', 'chika', 'crappi', 'ami', 'diego', 'humor', 'rotat', 'buff', 'hazard', 'music', 'contrast', 'certain', 'cabl', 'obtain', 'flex', 'snow', 'gs', 'identifi', 'unexpect', 'stash', 'pregnanc', \"c'\", 'duplic', 'dull', 'album', 'vertic', 'solut', 'totem', 'swallow', 'layout', 'essenti', 'gold', 'rapid', 'touch', 'je', 'folder', 'striker', 'deiti', 'post', 'yeast', 'sand', 'hybrid', 'damn', 'pronounc', 'versatil', 'pardon', 'virtu', 'ap', 'arya', 'evolv', 'burn', 'fountain', 'threaten', 'rx', 'soooo', 'captain', 'se', 'e.', 'chang', 'consult', 'plain', 'insult', 'wipe', 'custodi', 'lube', 'confus', 'survivor', 'argentina', 'instanc', 'comedi', 'screenshot', 'ironi', 'fl', 'loser', 'ish', 'cool', 'park', 'enemi', 'heroin', 'picki', 'ai', 'exactli', 'bruh', 'gentli', 'iirc', 'metal', 'viral', 'gun', 'warp', 'odd', 'suddenli', 'whisper', 'inevit', 'mainli', 'unfair', 'middleman', 'winter', 'assert', 'hindsight', 'favorit', 'dm', 'rerol', 'v3', 'grate', 'countri', 'riski', 'counter', 'constitut', 'sort', 'dear', 'seat', 'bypass', 'horizon', 'interfer', 'equip', 'straw', 'foil', 'spite', 'foul', 'brush', 'heritag', 'tldr', 'string', 'lean', 'soda', 'mike', 'peopl', 'behavior', 'battl', 'prestig', 'clunki', 'flock', 'nocontext', 'pacif', 'exercis', 'debt', 'perspect', 'may', 'illinoi', 'afford', 'neighborhood', 'christ', 'potenti', 'rugbi', 'donor', 'dine', 'around', 'delet', 'vagina', 'dota', 'confer', 'token', 'amd', 'pact', 'mar', 'teas', 'log', 'surrend', 'eso', 'categori', 'winrat', 'ev', 'slur', 'nuke', 'divin', 'weight', 'len', 'awkward', 'ipad', 'coast', 'internet', 'side', 'usa', 'stealth', 'him', 'en', 'grip', 'act', 'inaccur', 'council', 'boomer', 'minimum', 'lectur', 'vent', 'pop', 'dumb', 'workplac', 'radiu', 'environment', 'uncomfort', 'johnson', 'shooter', 'panti', 'honest', 'skip', 'team', 'prompt', 'kitti', 'verbal', 'arrog', 'bread', 'jet', 'rep', 'mistaken', 'shotgun', 'inappropri', 'san', 'golden', 'asia', 'frozen', 'spice', 'hip', 'cognit', 'utterli', 'essenc', 'incom', 'drama', 'mount', 'deal', 'presidenti', 'sham', 'accur', 'bruce', 'pill', 'sudden', 'screw', 'distort', 'justifi', 'decid', 'lb', 'gambl', 'feasibl', 'headshot', 'unban', 'run', 'club', 'stereotyp', 'trainer', 'someth', 'degrad', 'hell', 'exhaust', 'listen', 'mail', 'lax', 'core', 'conscious', 'shadow', 'warlock', 'simplifi', 'is', 'page', 'mix', 'no', 'shini', 'btw', 'strip', 'bch', 'gag', 'extens', 'refer', 'accomplish', 'whale', 'suburb', 'neglig', 'color', 'nightmar', 'idol', 'idk', 'fals', 'profit', 'revel', 'cartoon', 'war', 'fanboy', 'backdoor', 'germani', 'awesom', 'duvet', 'malic', 'yadda', 'dorito', 'nonexist', 'tablet', 'medicar', 'diarrhea', 'repercuss', 'endorphin', 'telltal', 'outshin', 'zinc', 'mytholog', 'tailbon', 'crest', 'ach', 'goodnight', 'selfless', 'lonli', 'enzym', 'menac', 'chatroom', 'frantic', 'paroxetin', 'mortifi', 'humil', 'outsourc', 'underperform', 'donkey', 'ebb', 'aunti', 'dtc', 'queasi', 'longev', 'signifi', 'dealership', 'armpit', 'claustrophob', 'bum', 'vraylar', 'neurochem', 'stressful', 'granni']\n"
     ]
    }
   ],
   "source": [
    "print (feature_show) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predY = svmModel.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predP = svmModel.predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(Dir+'/'+'predictedProbResults_2.txt', \"w\")\n",
    "count = 0\n",
    "for p in predP:\n",
    "    f.write(str(ids[count])+' '+str(round(p[1], 2))+'\\n')\n",
    "    count += 1 \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
