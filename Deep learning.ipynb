{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "path = 'D:/DEpression_project/GoogleNews-vectors-negative300.bin'\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(count:2990524, index:9476)\n"
     ]
    }
   ],
   "source": [
    "for k,v in model.vocab.items():\n",
    "    if k == 'depressed':\n",
    "        print (v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(3000000, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bao\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Bao\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\Bao\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-a356211584c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print (type(model.syn0))\n",
    "print (model.syn0.shape)\n",
    "print (len(model.wv.index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bao\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Bao\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.06298828,  0.09716797, -0.3203125 ,  0.10742188, -0.19140625,\n",
       "        0.23730469, -0.06494141,  0.08007812,  0.390625  ,  0.04272461,\n",
       "       -0.00311279, -0.01818848, -0.05273438, -0.0025177 ,  0.04077148,\n",
       "       -0.06347656, -0.13476562,  0.23046875,  0.23339844, -0.19140625,\n",
       "        0.03295898, -0.06347656,  0.03930664, -0.01843262,  0.05053711,\n",
       "        0.0057373 , -0.03466797,  0.06933594,  0.12207031, -0.01403809,\n",
       "        0.10400391,  0.00093079, -0.04370117, -0.10107422,  0.00488281,\n",
       "        0.08691406, -0.15625   , -0.11767578, -0.16601562,  0.09521484,\n",
       "        0.1875    ,  0.05004883,  0.17871094, -0.10498047,  0.01623535,\n",
       "       -0.07421875, -0.11279297,  0.05029297, -0.0480957 , -0.02026367,\n",
       "       -0.12060547, -0.0324707 , -0.40625   ,  0.28125   ,  0.07324219,\n",
       "       -0.19238281, -0.11767578,  0.08496094, -0.02087402, -0.07080078,\n",
       "       -0.00390625,  0.14550781, -0.2734375 ,  0.07128906,  0.02563477,\n",
       "        0.05664062,  0.04882812, -0.05249023,  0.01403809, -0.18066406,\n",
       "        0.01599121, -0.32617188,  0.07177734,  0.16699219, -0.18066406,\n",
       "        0.12792969,  0.08642578,  0.24609375,  0.22167969,  0.12597656,\n",
       "        0.14453125,  0.06933594,  0.0189209 , -0.01513672, -0.0222168 ,\n",
       "        0.16992188,  0.03759766,  0.33203125, -0.19042969, -0.31835938,\n",
       "        0.05249023,  0.09277344,  0.08300781, -0.18554688, -0.12988281,\n",
       "        0.11328125,  0.47070312, -0.0625    ,  0.02246094, -0.17480469,\n",
       "        0.01116943, -0.09033203, -0.109375  ,  0.09033203,  0.20605469,\n",
       "       -0.15039062, -0.24609375, -0.21191406,  0.13574219, -0.10253906,\n",
       "        0.22167969, -0.26757812,  0.25195312, -0.17871094,  0.16796875,\n",
       "       -0.10693359,  0.12988281, -0.234375  ,  0.40429688,  0.01574707,\n",
       "        0.07080078,  0.11425781, -0.25      , -0.25195312, -0.10742188,\n",
       "       -0.16894531,  0.08740234, -0.11767578,  0.04907227,  0.04394531,\n",
       "       -0.00350952,  0.18847656, -0.03881836,  0.17382812,  0.08349609,\n",
       "       -0.23828125,  0.26757812,  0.16210938, -0.05175781,  0.03857422,\n",
       "       -0.05566406, -0.05834961,  0.03320312,  0.10595703, -0.08154297,\n",
       "        0.01080322, -0.1953125 ,  0.17285156, -0.20605469, -0.10058594,\n",
       "        0.12060547,  0.05712891,  0.20214844,  0.4765625 , -0.20019531,\n",
       "        0.04394531, -0.16210938, -0.01312256,  0.07519531,  0.08447266,\n",
       "        0.16601562,  0.25390625,  0.18066406,  0.22753906,  0.04174805,\n",
       "        0.07617188,  0.11962891,  0.08935547,  0.19042969,  0.03710938,\n",
       "       -0.20410156, -0.15917969, -0.11669922, -0.140625  ,  0.00601196,\n",
       "        0.06347656,  0.10498047, -0.13574219, -0.09326172,  0.09960938,\n",
       "        0.07324219,  0.05566406, -0.171875  ,  0.13476562,  0.08691406,\n",
       "       -0.12988281, -0.05932617,  0.39453125,  0.00183105,  0.00585938,\n",
       "       -0.18945312, -0.375     , -0.14257812,  0.03686523,  0.06738281,\n",
       "        0.17773438,  0.09082031,  0.1015625 , -0.05883789,  0.16894531,\n",
       "       -0.25585938,  0.19042969, -0.29492188, -0.47851562,  0.04125977,\n",
       "       -0.23535156, -0.08300781, -0.14453125, -0.30273438, -0.12792969,\n",
       "       -0.14453125, -0.00245667,  0.00897217,  0.23242188, -0.00524902,\n",
       "        0.06347656,  0.24609375, -0.01623535, -0.08203125, -0.03833008,\n",
       "        0.08837891,  0.24511719, -0.14160156,  0.12792969,  0.13476562,\n",
       "       -0.3359375 ,  0.07666016, -0.4296875 , -0.18359375,  0.03320312,\n",
       "       -0.0456543 , -0.10498047, -0.14941406, -0.078125  , -0.04736328,\n",
       "       -0.08300781,  0.13769531, -0.15917969, -0.03417969,  0.06005859,\n",
       "        0.22949219, -0.20605469, -0.07714844, -0.02270508, -0.09960938,\n",
       "        0.05517578, -0.12255859,  0.00317383, -0.12792969, -0.20996094,\n",
       "        0.19726562, -0.23925781,  0.38671875,  0.06225586, -0.20703125,\n",
       "       -0.18457031,  0.01416016, -0.10595703,  0.31835938,  0.01953125,\n",
       "       -0.17773438, -0.16308594, -0.09423828,  0.16992188,  0.13574219,\n",
       "        0.21191406, -0.09033203, -0.11328125,  0.15234375,  0.08740234,\n",
       "       -0.01733398,  0.33789062,  0.41992188, -0.10449219, -0.2890625 ,\n",
       "       -0.2578125 , -0.21386719,  0.16503906, -0.13671875,  0.07519531,\n",
       "        0.1875    , -0.13867188, -0.05517578, -0.24316406,  0.16699219,\n",
       "       -0.078125  ,  0.07617188,  0.03588867,  0.26953125, -0.06298828,\n",
       "       -0.03417969,  0.14257812, -0.01623535, -0.07519531, -0.15332031,\n",
       "        0.02453613, -0.22753906,  0.26757812,  0.29101562, -0.15527344],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0[9476]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bao\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "#zip the two lists containing vectors and words\n",
    "embd_mat = zip(model.index2word, model.syn0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word vectors are stored in a KeyedVectors instance in model.wv. This separates the read-only word vector lookup operations in KeyedVectors from the training code in Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bao\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.06298828,  0.09716797, -0.3203125 ,  0.10742188, -0.19140625,\n",
       "        0.23730469, -0.06494141,  0.08007812,  0.390625  ,  0.04272461,\n",
       "       -0.00311279, -0.01818848, -0.05273438, -0.0025177 ,  0.04077148,\n",
       "       -0.06347656, -0.13476562,  0.23046875,  0.23339844, -0.19140625,\n",
       "        0.03295898, -0.06347656,  0.03930664, -0.01843262,  0.05053711,\n",
       "        0.0057373 , -0.03466797,  0.06933594,  0.12207031, -0.01403809,\n",
       "        0.10400391,  0.00093079, -0.04370117, -0.10107422,  0.00488281,\n",
       "        0.08691406, -0.15625   , -0.11767578, -0.16601562,  0.09521484,\n",
       "        0.1875    ,  0.05004883,  0.17871094, -0.10498047,  0.01623535,\n",
       "       -0.07421875, -0.11279297,  0.05029297, -0.0480957 , -0.02026367,\n",
       "       -0.12060547, -0.0324707 , -0.40625   ,  0.28125   ,  0.07324219,\n",
       "       -0.19238281, -0.11767578,  0.08496094, -0.02087402, -0.07080078,\n",
       "       -0.00390625,  0.14550781, -0.2734375 ,  0.07128906,  0.02563477,\n",
       "        0.05664062,  0.04882812, -0.05249023,  0.01403809, -0.18066406,\n",
       "        0.01599121, -0.32617188,  0.07177734,  0.16699219, -0.18066406,\n",
       "        0.12792969,  0.08642578,  0.24609375,  0.22167969,  0.12597656,\n",
       "        0.14453125,  0.06933594,  0.0189209 , -0.01513672, -0.0222168 ,\n",
       "        0.16992188,  0.03759766,  0.33203125, -0.19042969, -0.31835938,\n",
       "        0.05249023,  0.09277344,  0.08300781, -0.18554688, -0.12988281,\n",
       "        0.11328125,  0.47070312, -0.0625    ,  0.02246094, -0.17480469,\n",
       "        0.01116943, -0.09033203, -0.109375  ,  0.09033203,  0.20605469,\n",
       "       -0.15039062, -0.24609375, -0.21191406,  0.13574219, -0.10253906,\n",
       "        0.22167969, -0.26757812,  0.25195312, -0.17871094,  0.16796875,\n",
       "       -0.10693359,  0.12988281, -0.234375  ,  0.40429688,  0.01574707,\n",
       "        0.07080078,  0.11425781, -0.25      , -0.25195312, -0.10742188,\n",
       "       -0.16894531,  0.08740234, -0.11767578,  0.04907227,  0.04394531,\n",
       "       -0.00350952,  0.18847656, -0.03881836,  0.17382812,  0.08349609,\n",
       "       -0.23828125,  0.26757812,  0.16210938, -0.05175781,  0.03857422,\n",
       "       -0.05566406, -0.05834961,  0.03320312,  0.10595703, -0.08154297,\n",
       "        0.01080322, -0.1953125 ,  0.17285156, -0.20605469, -0.10058594,\n",
       "        0.12060547,  0.05712891,  0.20214844,  0.4765625 , -0.20019531,\n",
       "        0.04394531, -0.16210938, -0.01312256,  0.07519531,  0.08447266,\n",
       "        0.16601562,  0.25390625,  0.18066406,  0.22753906,  0.04174805,\n",
       "        0.07617188,  0.11962891,  0.08935547,  0.19042969,  0.03710938,\n",
       "       -0.20410156, -0.15917969, -0.11669922, -0.140625  ,  0.00601196,\n",
       "        0.06347656,  0.10498047, -0.13574219, -0.09326172,  0.09960938,\n",
       "        0.07324219,  0.05566406, -0.171875  ,  0.13476562,  0.08691406,\n",
       "       -0.12988281, -0.05932617,  0.39453125,  0.00183105,  0.00585938,\n",
       "       -0.18945312, -0.375     , -0.14257812,  0.03686523,  0.06738281,\n",
       "        0.17773438,  0.09082031,  0.1015625 , -0.05883789,  0.16894531,\n",
       "       -0.25585938,  0.19042969, -0.29492188, -0.47851562,  0.04125977,\n",
       "       -0.23535156, -0.08300781, -0.14453125, -0.30273438, -0.12792969,\n",
       "       -0.14453125, -0.00245667,  0.00897217,  0.23242188, -0.00524902,\n",
       "        0.06347656,  0.24609375, -0.01623535, -0.08203125, -0.03833008,\n",
       "        0.08837891,  0.24511719, -0.14160156,  0.12792969,  0.13476562,\n",
       "       -0.3359375 ,  0.07666016, -0.4296875 , -0.18359375,  0.03320312,\n",
       "       -0.0456543 , -0.10498047, -0.14941406, -0.078125  , -0.04736328,\n",
       "       -0.08300781,  0.13769531, -0.15917969, -0.03417969,  0.06005859,\n",
       "        0.22949219, -0.20605469, -0.07714844, -0.02270508, -0.09960938,\n",
       "        0.05517578, -0.12255859,  0.00317383, -0.12792969, -0.20996094,\n",
       "        0.19726562, -0.23925781,  0.38671875,  0.06225586, -0.20703125,\n",
       "       -0.18457031,  0.01416016, -0.10595703,  0.31835938,  0.01953125,\n",
       "       -0.17773438, -0.16308594, -0.09423828,  0.16992188,  0.13574219,\n",
       "        0.21191406, -0.09033203, -0.11328125,  0.15234375,  0.08740234,\n",
       "       -0.01733398,  0.33789062,  0.41992188, -0.10449219, -0.2890625 ,\n",
       "       -0.2578125 , -0.21386719,  0.16503906, -0.13671875,  0.07519531,\n",
       "        0.1875    , -0.13867188, -0.05517578, -0.24316406,  0.16699219,\n",
       "       -0.078125  ,  0.07617188,  0.03588867,  0.26953125, -0.06298828,\n",
       "       -0.03417969,  0.14257812, -0.01623535, -0.07519531, -0.15332031,\n",
       "        0.02453613, -0.22753906,  0.26757812,  0.29101562, -0.15527344],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['depressed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bao\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5463774512425825"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('depression', 'bipolar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embeddings dimension is 300\n",
      "The vocabulary size is 3000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bao\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n"
     ]
    }
   ],
   "source": [
    "path = 'D:/DEpression_project/GoogleNews-vectors-negative300.bin'\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "\n",
    "embd = model.syn0   # embd is numpy array\n",
    "embd_dim = embd.shape[1]   # should be 300\n",
    "vocab = model.vocab.keys()\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print ('The embeddings dimension is '+ str(embd_dim))\n",
    "print ('The vocabulary size is '+ str(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading in the vectors, we need to use them to initialize W of the embedding layer in your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embd_dim]),\n",
    "                trainable=False, name=\"W\")\n",
    "embd_placeholder = tf.placeholder(tf.float32, [vocab_size, embd_dim])\n",
    "embd_init = W.assign(embd_placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a session and initialize global variables, run the embedding_init operation by feeding in the 2-D array embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.1291504e-03, -8.9645386e-04,  3.1852722e-04, ...,\n",
       "        -1.5640259e-03, -1.2302399e-04, -8.6307526e-05],\n",
       "       [ 7.0312500e-02,  8.6914062e-02,  8.7890625e-02, ...,\n",
       "        -4.7607422e-02,  1.4465332e-02, -6.2500000e-02],\n",
       "       [-1.1779785e-02, -4.7363281e-02,  4.4677734e-02, ...,\n",
       "         7.1289062e-02, -3.4912109e-02,  2.4169922e-02],\n",
       "       ...,\n",
       "       [-1.9653320e-02, -9.0820312e-02, -1.9409180e-02, ...,\n",
       "        -1.6357422e-02, -1.3427734e-02,  4.6630859e-02],\n",
       "       [ 3.2714844e-02, -3.2226562e-02,  3.6132812e-02, ...,\n",
       "        -8.8500977e-03,  2.6977539e-02,  1.9042969e-02],\n",
       "       [ 4.5166016e-02, -4.5166016e-02, -3.9367676e-03, ...,\n",
       "         7.9589844e-02,  7.2265625e-02,  1.3000488e-02]], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(embd_init, feed_dict={embd_placeholder: embd})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have raw documents, the first thing you need to do is to build a vocabulary, which will map each word into an id. So we would choose the pre-trained model when we build the vocabulary: word-id maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prepocess our own data\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "hm_lines = 10000000\n",
    "\n",
    "def create_lexicon(pos, neg):\n",
    "    lexicon = []\n",
    "    for fi in [pos, neg]:\n",
    "        with open(fi, 'r') as f:\n",
    "            contents = f.readlines()\n",
    "            for line in contents[:hm_lines]:\n",
    "                all_words = word_tokenize(line.lower())\n",
    "                lexicon += list(all_words)\n",
    "                \n",
    "    w_counts = Counter(lexicon)\n",
    "    \n",
    "    lexicon2 = []\n",
    "    for w in w_counts:\n",
    "        if 1000 > w_counts[w] > 50:\n",
    "            lexicon2.append(w)\n",
    "        \n",
    "    return lexicon2    \n",
    "\n",
    "def sample_handling(sample, lexicon, label):\n",
    "    featureset = []\n",
    "    \n",
    "    with open(sample, 'r') as f:\n",
    "        contents = f.readlines()\n",
    "        for line in contents[:hm_lines]:\n",
    "            current_words = word_tokenize(line.lower())\n",
    "            features = np.zeros(len(lexicon))\n",
    "            for word in current_words:\n",
    "                if word.lower() in lexicon:\n",
    "                    index_value = lexicon.index(word.lower())\n",
    "                    features[index_value] += 1.0\n",
    "            features = list(features)\n",
    "            featureset.append([features, label])\n",
    "            \n",
    "    return featureset\n",
    "\n",
    "def creat_featuresets_and_labels(pos, neg, test_size=0.2):\n",
    "    lexicon = create_lexicon(pos, neg)\n",
    "    features = []\n",
    "    features += sample_handling('pos.txt', lexicon, [1,0])\n",
    "    features += sample_handling('neg.txt', lexicon, [0,1])\n",
    "    random.shuffle(features)\n",
    "    \n",
    "    features = np.array(features)\n",
    "    \n",
    "    testing_size = int(test_size*len(features))\n",
    "    \n",
    "    train_x = list(features[:, 0][:-testing_size])\n",
    "    train_y = list(features[:, 1][:-testing_size])\n",
    "    \n",
    "    test_x = list(features[:, 0][-testing_size:])\n",
    "    test_y = list(features[:, 1][-testing_size:])\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    train_x, train_y, test_x, test_y = creat_featuresets_and_labels('pos.txt', 'neg.txt')\n",
    "    with open('sentiment_set.pickle', 'wb') as f:\n",
    "        pickle.dump([train_x, train_y, test_x, test_y], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-b9098f179fec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcreate_sentiment_featuresets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcreat_featuresets_and_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreat_featuresets_and_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pos.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'neg.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Bao\\Documents\\Depression project\\create_sentiment_featuresets.py\u001b[0m in \u001b[0;36mcreat_featuresets_and_labels\u001b[0;34m(pos, neg, test_size)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreat_featuresets_and_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mlexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_lexicon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msample_handling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pos.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msample_handling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'neg.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Bao\\Documents\\Depression project\\create_sentiment_featuresets.py\u001b[0m in \u001b[0;36msample_handling\u001b[0;34m(sample, lexicon, label)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mcontents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mhm_lines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mcurrent_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlexicon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcurrent_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#from create_sentiment_featuresets import creat_featuresets_and_labels\n",
    "#train_x, train_y, test_x, test_y = creat_featuresets_and_labels('pos.txt', 'neg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_off = open(\"sentiment_set.pickle\",\"rb\")\n",
    "train_x, train_y, test_x, test_y = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407\n"
     ]
    }
   ],
   "source": [
    "doc_length = []\n",
    "for i in train_x:\n",
    "    doc_length.append(len(i))\n",
    "max_doc_length = max(doc_length)\n",
    "print (max_doc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_x = tf.placeholder(tf.int32, [None, len(train_x[0])], name=\"input_x\")\n",
    "#input_y = tf.placeholder(tf.float32, [None, num_classes = 2)], name=\"input_y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embd_chars = tf.nn.embedding_lookup(W, input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape input data\n",
    "x_unstack = tf.unstack(embedded_chars, len(train_x[0]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Tensor objects are not iterable when eager execution is not enabled. To iterate over this tensor use tf.map_fn.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-c4c3bfe81356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpretrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab_processor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;31m#transform inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_processor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Bao\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_document_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mWord\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mid\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \"\"\"\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[1;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m       \u001b[0mword_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_document_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Bao\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtokenizer\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0mper\u001b[0m \u001b[0meach\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m   \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m   \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[1;32myield\u001b[0m \u001b[0mTOKENIZER_RE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Bao\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m       raise TypeError(\n\u001b[0;32m--> 400\u001b[0;31m           \u001b[1;34m\"Tensor objects are not iterable when eager execution is not \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m           \"enabled. To iterate over this tensor use tf.map_fn.\")\n\u001b[1;32m    402\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Tensor objects are not iterable when eager execution is not enabled. To iterate over this tensor use tf.map_fn."
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib import learn\n",
    "#init vocab processor\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_doc_length)\n",
    "#fit the vocab from glove\n",
    "pretrain = vocab_processor.fit(vocab)\n",
    "#transform inputs\n",
    "x = np.array(list(vocab_processor.transform(input_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Creating the model\n",
    "path = 'D:/Depression_project/wiki-news-300d-1M.vec'\n",
    "model2 = KeyedVectors.load_word2vec_format(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use this link to see how to install tensorflow on Windows:\n",
    "https://www.tensorflow.org/install/install_windows\n",
    "\n",
    "If you want to use tenserflow in Jupyer Notebook, use the following:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholders for input, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_x = tf.placeholder(tf.float32, [None, doc_length], name=\"input_x\")\n",
    "input_y = tf.placeholder(tf.int32, [None, num_classes], name=\"input_y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use pre-trained embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_mat = "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
